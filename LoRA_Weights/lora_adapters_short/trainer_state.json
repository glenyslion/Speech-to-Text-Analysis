{
  "best_global_step": null,
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 40.0,
  "eval_steps": 500,
  "global_step": 880,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.22727272727272727,
      "grad_norm": NaN,
      "learning_rate": 0.0004,
      "loss": 5.4014,
      "step": 5
    },
    {
      "epoch": 0.45454545454545453,
      "grad_norm": 1.1909198760986328,
      "learning_rate": 0.0007,
      "loss": 5.3164,
      "step": 10
    },
    {
      "epoch": 0.6818181818181818,
      "grad_norm": 0.9228965044021606,
      "learning_rate": 0.0009977011494252874,
      "loss": 4.2974,
      "step": 15
    },
    {
      "epoch": 0.9090909090909091,
      "grad_norm": 0.3737453818321228,
      "learning_rate": 0.0009919540229885058,
      "loss": 3.5047,
      "step": 20
    },
    {
      "epoch": 1.1363636363636362,
      "grad_norm": 0.43776097893714905,
      "learning_rate": 0.000986206896551724,
      "loss": 3.0723,
      "step": 25
    },
    {
      "epoch": 1.3636363636363638,
      "grad_norm": 0.8955353498458862,
      "learning_rate": 0.0009804597701149427,
      "loss": 3.3457,
      "step": 30
    },
    {
      "epoch": 1.5909090909090908,
      "grad_norm": 0.41797032952308655,
      "learning_rate": 0.0009747126436781609,
      "loss": 3.1171,
      "step": 35
    },
    {
      "epoch": 1.8181818181818183,
      "grad_norm": 0.5367476344108582,
      "learning_rate": 0.0009689655172413794,
      "loss": 3.0114,
      "step": 40
    },
    {
      "epoch": 2.0454545454545454,
      "grad_norm": 0.4926638603210449,
      "learning_rate": 0.0009632183908045977,
      "loss": 2.8464,
      "step": 45
    },
    {
      "epoch": 2.2727272727272725,
      "grad_norm": 0.35930752754211426,
      "learning_rate": 0.000957471264367816,
      "loss": 2.9119,
      "step": 50
    },
    {
      "epoch": 2.5,
      "grad_norm": 0.47602376341819763,
      "learning_rate": 0.0009517241379310345,
      "loss": 2.7168,
      "step": 55
    },
    {
      "epoch": 2.7272727272727275,
      "grad_norm": 0.36868128180503845,
      "learning_rate": 0.0009459770114942529,
      "loss": 2.6673,
      "step": 60
    },
    {
      "epoch": 2.9545454545454546,
      "grad_norm": 0.4341461658477783,
      "learning_rate": 0.0009402298850574713,
      "loss": 2.8213,
      "step": 65
    },
    {
      "epoch": 3.1818181818181817,
      "grad_norm": 0.4049513638019562,
      "learning_rate": 0.0009344827586206896,
      "loss": 2.6559,
      "step": 70
    },
    {
      "epoch": 3.409090909090909,
      "grad_norm": 0.4269905984401703,
      "learning_rate": 0.0009287356321839081,
      "loss": 2.5783,
      "step": 75
    },
    {
      "epoch": 3.6363636363636362,
      "grad_norm": 0.44332370162010193,
      "learning_rate": 0.0009229885057471265,
      "loss": 2.6135,
      "step": 80
    },
    {
      "epoch": 3.8636363636363638,
      "grad_norm": 0.38571229577064514,
      "learning_rate": 0.0009172413793103448,
      "loss": 2.6915,
      "step": 85
    },
    {
      "epoch": 4.090909090909091,
      "grad_norm": 0.5165812969207764,
      "learning_rate": 0.0009114942528735632,
      "loss": 2.5278,
      "step": 90
    },
    {
      "epoch": 4.318181818181818,
      "grad_norm": 0.4350549876689911,
      "learning_rate": 0.0009057471264367815,
      "loss": 2.4952,
      "step": 95
    },
    {
      "epoch": 4.545454545454545,
      "grad_norm": 0.4257550537586212,
      "learning_rate": 0.0009000000000000001,
      "loss": 2.7501,
      "step": 100
    },
    {
      "epoch": 4.7727272727272725,
      "grad_norm": 0.4389621615409851,
      "learning_rate": 0.0008942528735632184,
      "loss": 2.55,
      "step": 105
    },
    {
      "epoch": 5.0,
      "grad_norm": 0.467193603515625,
      "learning_rate": 0.0008885057471264368,
      "loss": 2.4236,
      "step": 110
    },
    {
      "epoch": 5.2272727272727275,
      "grad_norm": 0.42268115282058716,
      "learning_rate": 0.0008827586206896551,
      "loss": 2.4725,
      "step": 115
    },
    {
      "epoch": 5.454545454545454,
      "grad_norm": 0.4255833625793457,
      "learning_rate": 0.0008770114942528737,
      "loss": 2.5926,
      "step": 120
    },
    {
      "epoch": 5.681818181818182,
      "grad_norm": 0.45917823910713196,
      "learning_rate": 0.000871264367816092,
      "loss": 2.3514,
      "step": 125
    },
    {
      "epoch": 5.909090909090909,
      "grad_norm": 0.4875136613845825,
      "learning_rate": 0.0008655172413793103,
      "loss": 2.468,
      "step": 130
    },
    {
      "epoch": 6.136363636363637,
      "grad_norm": 0.4444945752620697,
      "learning_rate": 0.0008597701149425287,
      "loss": 2.4939,
      "step": 135
    },
    {
      "epoch": 6.363636363636363,
      "grad_norm": 0.443528413772583,
      "learning_rate": 0.0008540229885057472,
      "loss": 2.3908,
      "step": 140
    },
    {
      "epoch": 6.590909090909091,
      "grad_norm": 0.5004339218139648,
      "learning_rate": 0.0008482758620689656,
      "loss": 2.1579,
      "step": 145
    },
    {
      "epoch": 6.818181818181818,
      "grad_norm": 0.5167011022567749,
      "learning_rate": 0.0008425287356321839,
      "loss": 2.5034,
      "step": 150
    },
    {
      "epoch": 7.045454545454546,
      "grad_norm": 0.44128090143203735,
      "learning_rate": 0.0008367816091954023,
      "loss": 2.5809,
      "step": 155
    },
    {
      "epoch": 7.2727272727272725,
      "grad_norm": 0.41137126088142395,
      "learning_rate": 0.0008310344827586206,
      "loss": 2.3425,
      "step": 160
    },
    {
      "epoch": 7.5,
      "grad_norm": 0.4539337158203125,
      "learning_rate": 0.0008252873563218391,
      "loss": 2.475,
      "step": 165
    },
    {
      "epoch": 7.7272727272727275,
      "grad_norm": 0.41731470823287964,
      "learning_rate": 0.0008195402298850575,
      "loss": 2.4085,
      "step": 170
    },
    {
      "epoch": 7.954545454545455,
      "grad_norm": 0.5102551579475403,
      "learning_rate": 0.0008137931034482758,
      "loss": 2.2825,
      "step": 175
    },
    {
      "epoch": 8.181818181818182,
      "grad_norm": 0.5205265879631042,
      "learning_rate": 0.0008080459770114942,
      "loss": 2.1358,
      "step": 180
    },
    {
      "epoch": 8.409090909090908,
      "grad_norm": 0.5564937591552734,
      "learning_rate": 0.0008022988505747127,
      "loss": 2.3709,
      "step": 185
    },
    {
      "epoch": 8.636363636363637,
      "grad_norm": 0.4974806606769562,
      "learning_rate": 0.0007965517241379311,
      "loss": 2.5087,
      "step": 190
    },
    {
      "epoch": 8.863636363636363,
      "grad_norm": 0.44642969965934753,
      "learning_rate": 0.0007908045977011494,
      "loss": 2.285,
      "step": 195
    },
    {
      "epoch": 9.090909090909092,
      "grad_norm": 0.49263685941696167,
      "learning_rate": 0.0007850574712643678,
      "loss": 2.335,
      "step": 200
    },
    {
      "epoch": 9.318181818181818,
      "grad_norm": 0.5306961536407471,
      "learning_rate": 0.0007793103448275863,
      "loss": 2.2547,
      "step": 205
    },
    {
      "epoch": 9.545454545454545,
      "grad_norm": 0.5496899485588074,
      "learning_rate": 0.0007735632183908046,
      "loss": 2.1996,
      "step": 210
    },
    {
      "epoch": 9.772727272727273,
      "grad_norm": 0.5314975380897522,
      "learning_rate": 0.000767816091954023,
      "loss": 2.3097,
      "step": 215
    },
    {
      "epoch": 10.0,
      "grad_norm": 0.576231837272644,
      "learning_rate": 0.0007620689655172413,
      "loss": 2.3797,
      "step": 220
    },
    {
      "epoch": 10.227272727272727,
      "grad_norm": 0.5052516460418701,
      "learning_rate": 0.0007563218390804599,
      "loss": 2.4025,
      "step": 225
    },
    {
      "epoch": 10.454545454545455,
      "grad_norm": 0.5187382698059082,
      "learning_rate": 0.0007505747126436782,
      "loss": 2.2178,
      "step": 230
    },
    {
      "epoch": 10.681818181818182,
      "grad_norm": 0.566274881362915,
      "learning_rate": 0.0007448275862068966,
      "loss": 2.1027,
      "step": 235
    },
    {
      "epoch": 10.909090909090908,
      "grad_norm": 0.5484368205070496,
      "learning_rate": 0.0007390804597701149,
      "loss": 2.3151,
      "step": 240
    },
    {
      "epoch": 11.136363636363637,
      "grad_norm": 0.508547306060791,
      "learning_rate": 0.0007333333333333333,
      "loss": 2.2776,
      "step": 245
    },
    {
      "epoch": 11.363636363636363,
      "grad_norm": 0.5637897849082947,
      "learning_rate": 0.0007275862068965518,
      "loss": 2.3064,
      "step": 250
    },
    {
      "epoch": 11.590909090909092,
      "grad_norm": 0.499541699886322,
      "learning_rate": 0.0007218390804597701,
      "loss": 2.0358,
      "step": 255
    },
    {
      "epoch": 11.818181818181818,
      "grad_norm": 0.563140332698822,
      "learning_rate": 0.0007160919540229885,
      "loss": 2.2625,
      "step": 260
    },
    {
      "epoch": 12.045454545454545,
      "grad_norm": 0.558681070804596,
      "learning_rate": 0.0007103448275862069,
      "loss": 2.2888,
      "step": 265
    },
    {
      "epoch": 12.272727272727273,
      "grad_norm": 0.5454575419425964,
      "learning_rate": 0.0007045977011494254,
      "loss": 2.1692,
      "step": 270
    },
    {
      "epoch": 12.5,
      "grad_norm": 0.47709664702415466,
      "learning_rate": 0.0006988505747126437,
      "loss": 2.2938,
      "step": 275
    },
    {
      "epoch": 12.727272727272727,
      "grad_norm": 0.5408332943916321,
      "learning_rate": 0.0006931034482758621,
      "loss": 2.1284,
      "step": 280
    },
    {
      "epoch": 12.954545454545455,
      "grad_norm": 0.5008873343467712,
      "learning_rate": 0.0006873563218390805,
      "loss": 2.0923,
      "step": 285
    },
    {
      "epoch": 13.181818181818182,
      "grad_norm": 0.5956692099571228,
      "learning_rate": 0.0006816091954022988,
      "loss": 2.3911,
      "step": 290
    },
    {
      "epoch": 13.409090909090908,
      "grad_norm": 0.5192707180976868,
      "learning_rate": 0.0006758620689655173,
      "loss": 2.1503,
      "step": 295
    },
    {
      "epoch": 13.636363636363637,
      "grad_norm": 1.3160046339035034,
      "learning_rate": 0.0006701149425287356,
      "loss": 2.2284,
      "step": 300
    },
    {
      "epoch": 13.863636363636363,
      "grad_norm": 0.5976094007492065,
      "learning_rate": 0.0006643678160919541,
      "loss": 2.2274,
      "step": 305
    },
    {
      "epoch": 14.090909090909092,
      "grad_norm": 0.5236828923225403,
      "learning_rate": 0.0006586206896551724,
      "loss": 1.9705,
      "step": 310
    },
    {
      "epoch": 14.318181818181818,
      "grad_norm": 0.6521918177604675,
      "learning_rate": 0.0006528735632183909,
      "loss": 2.1285,
      "step": 315
    },
    {
      "epoch": 14.545454545454545,
      "grad_norm": 0.6251901388168335,
      "learning_rate": 0.0006471264367816092,
      "loss": 2.1765,
      "step": 320
    },
    {
      "epoch": 14.772727272727273,
      "grad_norm": 0.5861321687698364,
      "learning_rate": 0.0006413793103448275,
      "loss": 2.156,
      "step": 325
    },
    {
      "epoch": 15.0,
      "grad_norm": 0.7138742804527283,
      "learning_rate": 0.000635632183908046,
      "loss": 2.271,
      "step": 330
    },
    {
      "epoch": 15.227272727272727,
      "grad_norm": 0.5644460320472717,
      "learning_rate": 0.0006298850574712644,
      "loss": 2.2068,
      "step": 335
    },
    {
      "epoch": 15.454545454545455,
      "grad_norm": 0.5718567967414856,
      "learning_rate": 0.0006241379310344828,
      "loss": 2.0617,
      "step": 340
    },
    {
      "epoch": 15.681818181818182,
      "grad_norm": 0.6290463805198669,
      "learning_rate": 0.0006183908045977011,
      "loss": 2.1154,
      "step": 345
    },
    {
      "epoch": 15.909090909090908,
      "grad_norm": 0.5751800537109375,
      "learning_rate": 0.0006126436781609196,
      "loss": 2.0939,
      "step": 350
    },
    {
      "epoch": 16.136363636363637,
      "grad_norm": 0.5371930599212646,
      "learning_rate": 0.0006068965517241379,
      "loss": 2.0273,
      "step": 355
    },
    {
      "epoch": 16.363636363636363,
      "grad_norm": 0.5725672245025635,
      "learning_rate": 0.0006011494252873564,
      "loss": 2.0326,
      "step": 360
    },
    {
      "epoch": 16.59090909090909,
      "grad_norm": 0.6728218793869019,
      "learning_rate": 0.0005954022988505747,
      "loss": 2.2443,
      "step": 365
    },
    {
      "epoch": 16.818181818181817,
      "grad_norm": 0.6858451962471008,
      "learning_rate": 0.0005896551724137931,
      "loss": 2.1515,
      "step": 370
    },
    {
      "epoch": 17.045454545454547,
      "grad_norm": 0.5693569779396057,
      "learning_rate": 0.0005839080459770115,
      "loss": 2.0834,
      "step": 375
    },
    {
      "epoch": 17.272727272727273,
      "grad_norm": 0.6575945615768433,
      "learning_rate": 0.0005781609195402299,
      "loss": 2.0585,
      "step": 380
    },
    {
      "epoch": 17.5,
      "grad_norm": 0.6394928097724915,
      "learning_rate": 0.0005724137931034483,
      "loss": 1.9156,
      "step": 385
    },
    {
      "epoch": 17.727272727272727,
      "grad_norm": 0.5302924513816833,
      "learning_rate": 0.0005666666666666667,
      "loss": 2.3016,
      "step": 390
    },
    {
      "epoch": 17.954545454545453,
      "grad_norm": 0.5880166888237,
      "learning_rate": 0.0005609195402298851,
      "loss": 2.0308,
      "step": 395
    },
    {
      "epoch": 18.181818181818183,
      "grad_norm": 0.5465237498283386,
      "learning_rate": 0.0005551724137931035,
      "loss": 2.1119,
      "step": 400
    },
    {
      "epoch": 18.40909090909091,
      "grad_norm": 0.6526380181312561,
      "learning_rate": 0.0005494252873563218,
      "loss": 2.0097,
      "step": 405
    },
    {
      "epoch": 18.636363636363637,
      "grad_norm": 0.5398396253585815,
      "learning_rate": 0.0005436781609195403,
      "loss": 2.1721,
      "step": 410
    },
    {
      "epoch": 18.863636363636363,
      "grad_norm": 0.6998754739761353,
      "learning_rate": 0.0005379310344827586,
      "loss": 2.0845,
      "step": 415
    },
    {
      "epoch": 19.09090909090909,
      "grad_norm": 0.7824384570121765,
      "learning_rate": 0.000532183908045977,
      "loss": 1.9921,
      "step": 420
    },
    {
      "epoch": 19.318181818181817,
      "grad_norm": 0.5520274639129639,
      "learning_rate": 0.0005264367816091954,
      "loss": 2.1149,
      "step": 425
    },
    {
      "epoch": 19.545454545454547,
      "grad_norm": 0.5598825216293335,
      "learning_rate": 0.0005206896551724139,
      "loss": 2.0539,
      "step": 430
    },
    {
      "epoch": 19.772727272727273,
      "grad_norm": 0.5900329351425171,
      "learning_rate": 0.0005149425287356322,
      "loss": 1.9959,
      "step": 435
    },
    {
      "epoch": 20.0,
      "grad_norm": 0.6847938895225525,
      "learning_rate": 0.0005091954022988506,
      "loss": 2.0199,
      "step": 440
    },
    {
      "epoch": 20.227272727272727,
      "grad_norm": 0.6461978554725647,
      "learning_rate": 0.000503448275862069,
      "loss": 2.0305,
      "step": 445
    },
    {
      "epoch": 20.454545454545453,
      "grad_norm": 0.5666112899780273,
      "learning_rate": 0.0004977011494252874,
      "loss": 2.0409,
      "step": 450
    },
    {
      "epoch": 20.681818181818183,
      "grad_norm": 0.6595930457115173,
      "learning_rate": 0.0004919540229885058,
      "loss": 2.0894,
      "step": 455
    },
    {
      "epoch": 20.90909090909091,
      "grad_norm": 0.9650394916534424,
      "learning_rate": 0.0004862068965517241,
      "loss": 2.128,
      "step": 460
    },
    {
      "epoch": 21.136363636363637,
      "grad_norm": 0.6823996901512146,
      "learning_rate": 0.0004804597701149425,
      "loss": 1.9813,
      "step": 465
    },
    {
      "epoch": 21.363636363636363,
      "grad_norm": 0.5620076060295105,
      "learning_rate": 0.0004747126436781609,
      "loss": 2.0362,
      "step": 470
    },
    {
      "epoch": 21.59090909090909,
      "grad_norm": 0.7085006237030029,
      "learning_rate": 0.0004689655172413793,
      "loss": 1.9799,
      "step": 475
    },
    {
      "epoch": 21.818181818181817,
      "grad_norm": 0.5626543164253235,
      "learning_rate": 0.0004632183908045977,
      "loss": 2.0363,
      "step": 480
    },
    {
      "epoch": 22.045454545454547,
      "grad_norm": 0.622366189956665,
      "learning_rate": 0.0004574712643678161,
      "loss": 2.0403,
      "step": 485
    },
    {
      "epoch": 22.272727272727273,
      "grad_norm": 0.5854565501213074,
      "learning_rate": 0.0004517241379310345,
      "loss": 1.8234,
      "step": 490
    },
    {
      "epoch": 22.5,
      "grad_norm": 0.7064765691757202,
      "learning_rate": 0.0004459770114942529,
      "loss": 2.0372,
      "step": 495
    },
    {
      "epoch": 22.727272727272727,
      "grad_norm": 0.6713911890983582,
      "learning_rate": 0.00044022988505747126,
      "loss": 2.0708,
      "step": 500
    },
    {
      "epoch": 22.954545454545453,
      "grad_norm": 0.8421231508255005,
      "learning_rate": 0.00043448275862068963,
      "loss": 2.0532,
      "step": 505
    },
    {
      "epoch": 23.181818181818183,
      "grad_norm": 0.6572939157485962,
      "learning_rate": 0.00042873563218390805,
      "loss": 2.0104,
      "step": 510
    },
    {
      "epoch": 23.40909090909091,
      "grad_norm": 0.594393253326416,
      "learning_rate": 0.0004229885057471264,
      "loss": 1.9074,
      "step": 515
    },
    {
      "epoch": 23.636363636363637,
      "grad_norm": 0.6096299290657043,
      "learning_rate": 0.00041724137931034485,
      "loss": 2.0559,
      "step": 520
    },
    {
      "epoch": 23.863636363636363,
      "grad_norm": 0.7269976735115051,
      "learning_rate": 0.0004114942528735632,
      "loss": 2.0122,
      "step": 525
    },
    {
      "epoch": 24.09090909090909,
      "grad_norm": 0.7204922437667847,
      "learning_rate": 0.00040574712643678165,
      "loss": 1.9762,
      "step": 530
    },
    {
      "epoch": 24.318181818181817,
      "grad_norm": 0.6401975154876709,
      "learning_rate": 0.0004,
      "loss": 2.1013,
      "step": 535
    },
    {
      "epoch": 24.545454545454547,
      "grad_norm": 0.6544498205184937,
      "learning_rate": 0.0003942528735632184,
      "loss": 1.9356,
      "step": 540
    },
    {
      "epoch": 24.772727272727273,
      "grad_norm": 0.6020357012748718,
      "learning_rate": 0.00038850574712643676,
      "loss": 2.0729,
      "step": 545
    },
    {
      "epoch": 25.0,
      "grad_norm": 0.6762924194335938,
      "learning_rate": 0.0003827586206896552,
      "loss": 1.8465,
      "step": 550
    },
    {
      "epoch": 25.227272727272727,
      "grad_norm": 0.6634035706520081,
      "learning_rate": 0.00037701149425287356,
      "loss": 2.1471,
      "step": 555
    },
    {
      "epoch": 25.454545454545453,
      "grad_norm": 0.6128194332122803,
      "learning_rate": 0.000371264367816092,
      "loss": 1.7616,
      "step": 560
    },
    {
      "epoch": 25.681818181818183,
      "grad_norm": 0.6587138175964355,
      "learning_rate": 0.00036551724137931036,
      "loss": 1.9321,
      "step": 565
    },
    {
      "epoch": 25.90909090909091,
      "grad_norm": 0.6778273582458496,
      "learning_rate": 0.0003597701149425288,
      "loss": 2.0247,
      "step": 570
    },
    {
      "epoch": 26.136363636363637,
      "grad_norm": 0.742432177066803,
      "learning_rate": 0.00035402298850574715,
      "loss": 1.9884,
      "step": 575
    },
    {
      "epoch": 26.363636363636363,
      "grad_norm": 0.8058393001556396,
      "learning_rate": 0.0003482758620689655,
      "loss": 1.8664,
      "step": 580
    },
    {
      "epoch": 26.59090909090909,
      "grad_norm": 0.739429235458374,
      "learning_rate": 0.0003425287356321839,
      "loss": 1.946,
      "step": 585
    },
    {
      "epoch": 26.818181818181817,
      "grad_norm": 0.7351104617118835,
      "learning_rate": 0.0003367816091954023,
      "loss": 2.0493,
      "step": 590
    },
    {
      "epoch": 27.045454545454547,
      "grad_norm": 0.6762824058532715,
      "learning_rate": 0.0003310344827586207,
      "loss": 1.8816,
      "step": 595
    },
    {
      "epoch": 27.272727272727273,
      "grad_norm": 0.5674661993980408,
      "learning_rate": 0.00032528735632183906,
      "loss": 1.8834,
      "step": 600
    },
    {
      "epoch": 27.5,
      "grad_norm": 0.738339364528656,
      "learning_rate": 0.0003195402298850575,
      "loss": 1.8584,
      "step": 605
    },
    {
      "epoch": 27.727272727272727,
      "grad_norm": 0.9216142296791077,
      "learning_rate": 0.00031379310344827586,
      "loss": 2.0007,
      "step": 610
    },
    {
      "epoch": 27.954545454545453,
      "grad_norm": 0.6313605308532715,
      "learning_rate": 0.0003080459770114943,
      "loss": 1.9872,
      "step": 615
    },
    {
      "epoch": 28.181818181818183,
      "grad_norm": 0.7236328721046448,
      "learning_rate": 0.0003022988505747126,
      "loss": 1.8694,
      "step": 620
    },
    {
      "epoch": 28.40909090909091,
      "grad_norm": 0.6900652050971985,
      "learning_rate": 0.00029655172413793103,
      "loss": 1.9771,
      "step": 625
    },
    {
      "epoch": 28.636363636363637,
      "grad_norm": 0.6814577579498291,
      "learning_rate": 0.0002908045977011494,
      "loss": 1.898,
      "step": 630
    },
    {
      "epoch": 28.863636363636363,
      "grad_norm": 0.7497552633285522,
      "learning_rate": 0.0002850574712643678,
      "loss": 2.1645,
      "step": 635
    },
    {
      "epoch": 29.09090909090909,
      "grad_norm": 0.6948437094688416,
      "learning_rate": 0.0002793103448275862,
      "loss": 1.9047,
      "step": 640
    },
    {
      "epoch": 29.318181818181817,
      "grad_norm": 0.8372136950492859,
      "learning_rate": 0.0002735632183908046,
      "loss": 2.0129,
      "step": 645
    },
    {
      "epoch": 29.545454545454547,
      "grad_norm": 0.6884730458259583,
      "learning_rate": 0.000267816091954023,
      "loss": 1.8031,
      "step": 650
    },
    {
      "epoch": 29.772727272727273,
      "grad_norm": 0.6704835891723633,
      "learning_rate": 0.0002620689655172414,
      "loss": 1.9713,
      "step": 655
    },
    {
      "epoch": 30.0,
      "grad_norm": 0.7697550058364868,
      "learning_rate": 0.00025632183908045974,
      "loss": 1.8449,
      "step": 660
    },
    {
      "epoch": 30.227272727272727,
      "grad_norm": 0.5930463075637817,
      "learning_rate": 0.00025057471264367816,
      "loss": 1.8841,
      "step": 665
    },
    {
      "epoch": 30.454545454545453,
      "grad_norm": 0.6881658434867859,
      "learning_rate": 0.00024482758620689653,
      "loss": 1.7644,
      "step": 670
    },
    {
      "epoch": 30.681818181818183,
      "grad_norm": 0.6462631821632385,
      "learning_rate": 0.00023908045977011493,
      "loss": 1.9408,
      "step": 675
    },
    {
      "epoch": 30.90909090909091,
      "grad_norm": 0.6753212809562683,
      "learning_rate": 0.00023333333333333333,
      "loss": 1.961,
      "step": 680
    },
    {
      "epoch": 31.136363636363637,
      "grad_norm": 0.6445794105529785,
      "learning_rate": 0.00022758620689655173,
      "loss": 2.1207,
      "step": 685
    },
    {
      "epoch": 31.363636363636363,
      "grad_norm": 0.6284151077270508,
      "learning_rate": 0.0002218390804597701,
      "loss": 1.8591,
      "step": 690
    },
    {
      "epoch": 31.59090909090909,
      "grad_norm": 0.7155041694641113,
      "learning_rate": 0.0002160919540229885,
      "loss": 1.7841,
      "step": 695
    },
    {
      "epoch": 31.818181818181817,
      "grad_norm": 0.7472851276397705,
      "learning_rate": 0.0002103448275862069,
      "loss": 2.0252,
      "step": 700
    },
    {
      "epoch": 32.04545454545455,
      "grad_norm": 0.7487460970878601,
      "learning_rate": 0.0002045977011494253,
      "loss": 1.9186,
      "step": 705
    },
    {
      "epoch": 32.27272727272727,
      "grad_norm": 0.7253642082214355,
      "learning_rate": 0.00019885057471264367,
      "loss": 1.8353,
      "step": 710
    },
    {
      "epoch": 32.5,
      "grad_norm": 0.6434327960014343,
      "learning_rate": 0.00019310344827586207,
      "loss": 1.8404,
      "step": 715
    },
    {
      "epoch": 32.72727272727273,
      "grad_norm": 0.672656238079071,
      "learning_rate": 0.00018735632183908046,
      "loss": 1.8957,
      "step": 720
    },
    {
      "epoch": 32.95454545454545,
      "grad_norm": 0.5840516090393066,
      "learning_rate": 0.00018160919540229886,
      "loss": 1.8771,
      "step": 725
    },
    {
      "epoch": 33.18181818181818,
      "grad_norm": 0.8028781414031982,
      "learning_rate": 0.00017586206896551723,
      "loss": 1.8811,
      "step": 730
    },
    {
      "epoch": 33.40909090909091,
      "grad_norm": 0.7332091927528381,
      "learning_rate": 0.00017011494252873563,
      "loss": 1.8949,
      "step": 735
    },
    {
      "epoch": 33.63636363636363,
      "grad_norm": 0.7727706432342529,
      "learning_rate": 0.00016436781609195403,
      "loss": 1.885,
      "step": 740
    },
    {
      "epoch": 33.86363636363637,
      "grad_norm": 0.6661830544471741,
      "learning_rate": 0.00015862068965517243,
      "loss": 1.8338,
      "step": 745
    },
    {
      "epoch": 34.09090909090909,
      "grad_norm": 0.6896529197692871,
      "learning_rate": 0.0001528735632183908,
      "loss": 1.9578,
      "step": 750
    },
    {
      "epoch": 34.31818181818182,
      "grad_norm": 0.7903231382369995,
      "learning_rate": 0.0001471264367816092,
      "loss": 1.9142,
      "step": 755
    },
    {
      "epoch": 34.54545454545455,
      "grad_norm": 0.8180328011512756,
      "learning_rate": 0.0001413793103448276,
      "loss": 1.9804,
      "step": 760
    },
    {
      "epoch": 34.77272727272727,
      "grad_norm": 0.6904110312461853,
      "learning_rate": 0.000135632183908046,
      "loss": 1.8914,
      "step": 765
    },
    {
      "epoch": 35.0,
      "grad_norm": 0.8191089630126953,
      "learning_rate": 0.00012988505747126437,
      "loss": 1.7375,
      "step": 770
    },
    {
      "epoch": 35.22727272727273,
      "grad_norm": 0.6590206027030945,
      "learning_rate": 0.00012413793103448277,
      "loss": 1.8639,
      "step": 775
    },
    {
      "epoch": 35.45454545454545,
      "grad_norm": 0.7246367931365967,
      "learning_rate": 0.00011839080459770115,
      "loss": 2.0184,
      "step": 780
    },
    {
      "epoch": 35.68181818181818,
      "grad_norm": 0.7555320262908936,
      "learning_rate": 0.00011264367816091954,
      "loss": 1.8567,
      "step": 785
    },
    {
      "epoch": 35.90909090909091,
      "grad_norm": 0.6869866847991943,
      "learning_rate": 0.00010689655172413793,
      "loss": 1.767,
      "step": 790
    },
    {
      "epoch": 36.13636363636363,
      "grad_norm": 0.7746948003768921,
      "learning_rate": 0.00010114942528735632,
      "loss": 1.9256,
      "step": 795
    },
    {
      "epoch": 36.36363636363637,
      "grad_norm": 0.6932455897331238,
      "learning_rate": 9.540229885057472e-05,
      "loss": 1.9132,
      "step": 800
    },
    {
      "epoch": 36.59090909090909,
      "grad_norm": 0.7099016904830933,
      "learning_rate": 8.96551724137931e-05,
      "loss": 1.8052,
      "step": 805
    },
    {
      "epoch": 36.81818181818182,
      "grad_norm": 0.836531400680542,
      "learning_rate": 8.39080459770115e-05,
      "loss": 1.8391,
      "step": 810
    },
    {
      "epoch": 37.04545454545455,
      "grad_norm": 0.6866863965988159,
      "learning_rate": 7.816091954022989e-05,
      "loss": 1.925,
      "step": 815
    },
    {
      "epoch": 37.27272727272727,
      "grad_norm": 0.6367815136909485,
      "learning_rate": 7.241379310344828e-05,
      "loss": 1.7992,
      "step": 820
    },
    {
      "epoch": 37.5,
      "grad_norm": 0.669143557548523,
      "learning_rate": 6.666666666666667e-05,
      "loss": 1.8667,
      "step": 825
    },
    {
      "epoch": 37.72727272727273,
      "grad_norm": 0.7452007532119751,
      "learning_rate": 6.0919540229885055e-05,
      "loss": 1.9534,
      "step": 830
    },
    {
      "epoch": 37.95454545454545,
      "grad_norm": 0.7034780383110046,
      "learning_rate": 5.5172413793103446e-05,
      "loss": 1.8669,
      "step": 835
    },
    {
      "epoch": 38.18181818181818,
      "grad_norm": 0.6990156173706055,
      "learning_rate": 4.942528735632184e-05,
      "loss": 1.9255,
      "step": 840
    },
    {
      "epoch": 38.40909090909091,
      "grad_norm": 0.8196009397506714,
      "learning_rate": 4.482758620689655e-05,
      "loss": 1.8049,
      "step": 845
    },
    {
      "epoch": 38.63636363636363,
      "grad_norm": 0.7748066782951355,
      "learning_rate": 3.908045977011494e-05,
      "loss": 1.7885,
      "step": 850
    },
    {
      "epoch": 38.86363636363637,
      "grad_norm": 0.8022216558456421,
      "learning_rate": 3.3333333333333335e-05,
      "loss": 1.9513,
      "step": 855
    },
    {
      "epoch": 39.09090909090909,
      "grad_norm": 0.6627917885780334,
      "learning_rate": 2.7586206896551723e-05,
      "loss": 1.8478,
      "step": 860
    },
    {
      "epoch": 39.31818181818182,
      "grad_norm": 0.7699491381645203,
      "learning_rate": 2.1839080459770115e-05,
      "loss": 1.7394,
      "step": 865
    },
    {
      "epoch": 39.54545454545455,
      "grad_norm": 0.7490374445915222,
      "learning_rate": 1.6091954022988507e-05,
      "loss": 2.0473,
      "step": 870
    },
    {
      "epoch": 39.77272727272727,
      "grad_norm": 0.7950474619865417,
      "learning_rate": 1.0344827586206897e-05,
      "loss": 1.8818,
      "step": 875
    },
    {
      "epoch": 40.0,
      "grad_norm": 0.7643016576766968,
      "learning_rate": 4.5977011494252875e-06,
      "loss": 1.8657,
      "step": 880
    }
  ],
  "logging_steps": 5,
  "max_steps": 880,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 40,
  "save_steps": 500,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": true
      },
      "attributes": {}
    }
  },
  "total_flos": 954590035968000.0,
  "train_batch_size": 4,
  "trial_name": null,
  "trial_params": null
}
