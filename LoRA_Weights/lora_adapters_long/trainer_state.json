{
  "best_global_step": null,
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 40.0,
  "eval_steps": 500,
  "global_step": 880,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.22727272727272727,
      "grad_norm": 1.7455941438674927,
      "learning_rate": 0.002,
      "loss": 8.64,
      "step": 5
    },
    {
      "epoch": 0.45454545454545453,
      "grad_norm": 0.45994365215301514,
      "learning_rate": 0.006999999999999999,
      "loss": 5.4026,
      "step": 10
    },
    {
      "epoch": 0.6818181818181818,
      "grad_norm": 0.4049854576587677,
      "learning_rate": 0.009977011494252874,
      "loss": 3.5069,
      "step": 15
    },
    {
      "epoch": 0.9090909090909091,
      "grad_norm": 0.42023393511772156,
      "learning_rate": 0.009919540229885058,
      "loss": 3.2388,
      "step": 20
    },
    {
      "epoch": 1.1363636363636362,
      "grad_norm": 0.43229591846466064,
      "learning_rate": 0.00986206896551724,
      "loss": 2.6283,
      "step": 25
    },
    {
      "epoch": 1.3636363636363638,
      "grad_norm": 0.4201546609401703,
      "learning_rate": 0.009804597701149427,
      "loss": 2.7318,
      "step": 30
    },
    {
      "epoch": 1.5909090909090908,
      "grad_norm": 0.4090896248817444,
      "learning_rate": 0.00974712643678161,
      "loss": 2.8073,
      "step": 35
    },
    {
      "epoch": 1.8181818181818183,
      "grad_norm": 0.40751028060913086,
      "learning_rate": 0.009689655172413793,
      "loss": 2.8517,
      "step": 40
    },
    {
      "epoch": 2.0454545454545454,
      "grad_norm": 0.447977215051651,
      "learning_rate": 0.009632183908045978,
      "loss": 2.5445,
      "step": 45
    },
    {
      "epoch": 2.2727272727272725,
      "grad_norm": 0.45378515124320984,
      "learning_rate": 0.00957471264367816,
      "loss": 2.6001,
      "step": 50
    },
    {
      "epoch": 2.5,
      "grad_norm": 0.44762200117111206,
      "learning_rate": 0.009517241379310344,
      "loss": 2.5492,
      "step": 55
    },
    {
      "epoch": 2.7272727272727275,
      "grad_norm": 0.3697223961353302,
      "learning_rate": 0.009459770114942529,
      "loss": 2.6136,
      "step": 60
    },
    {
      "epoch": 2.9545454545454546,
      "grad_norm": 0.4679391384124756,
      "learning_rate": 0.009402298850574713,
      "loss": 2.5397,
      "step": 65
    },
    {
      "epoch": 3.1818181818181817,
      "grad_norm": 0.799450695514679,
      "learning_rate": 0.009344827586206897,
      "loss": 2.361,
      "step": 70
    },
    {
      "epoch": 3.409090909090909,
      "grad_norm": 0.3826121687889099,
      "learning_rate": 0.009287356321839082,
      "loss": 2.486,
      "step": 75
    },
    {
      "epoch": 3.6363636363636362,
      "grad_norm": 0.4846819341182709,
      "learning_rate": 0.009229885057471264,
      "loss": 2.3533,
      "step": 80
    },
    {
      "epoch": 3.8636363636363638,
      "grad_norm": 0.3973711431026459,
      "learning_rate": 0.009172413793103448,
      "loss": 2.5418,
      "step": 85
    },
    {
      "epoch": 4.090909090909091,
      "grad_norm": 0.4548896849155426,
      "learning_rate": 0.009114942528735633,
      "loss": 2.1745,
      "step": 90
    },
    {
      "epoch": 4.318181818181818,
      "grad_norm": 0.37298867106437683,
      "learning_rate": 0.009057471264367815,
      "loss": 2.3745,
      "step": 95
    },
    {
      "epoch": 4.545454545454545,
      "grad_norm": 0.42019668221473694,
      "learning_rate": 0.009000000000000001,
      "loss": 2.3226,
      "step": 100
    },
    {
      "epoch": 4.7727272727272725,
      "grad_norm": 0.36832091212272644,
      "learning_rate": 0.008942528735632184,
      "loss": 2.3472,
      "step": 105
    },
    {
      "epoch": 5.0,
      "grad_norm": 0.5926884412765503,
      "learning_rate": 0.008885057471264368,
      "loss": 2.175,
      "step": 110
    },
    {
      "epoch": 5.2272727272727275,
      "grad_norm": 0.5672938227653503,
      "learning_rate": 0.008827586206896552,
      "loss": 2.0943,
      "step": 115
    },
    {
      "epoch": 5.454545454545454,
      "grad_norm": 0.43959149718284607,
      "learning_rate": 0.008770114942528736,
      "loss": 2.3448,
      "step": 120
    },
    {
      "epoch": 5.681818181818182,
      "grad_norm": 0.44972047209739685,
      "learning_rate": 0.00871264367816092,
      "loss": 2.3673,
      "step": 125
    },
    {
      "epoch": 5.909090909090909,
      "grad_norm": 0.5014464855194092,
      "learning_rate": 0.008655172413793103,
      "loss": 2.2927,
      "step": 130
    },
    {
      "epoch": 6.136363636363637,
      "grad_norm": 0.4956418573856354,
      "learning_rate": 0.008597701149425287,
      "loss": 2.0919,
      "step": 135
    },
    {
      "epoch": 6.363636363636363,
      "grad_norm": 1.1560674905776978,
      "learning_rate": 0.008540229885057472,
      "loss": 2.2629,
      "step": 140
    },
    {
      "epoch": 6.590909090909091,
      "grad_norm": 0.45256128907203674,
      "learning_rate": 0.008482758620689656,
      "loss": 2.1335,
      "step": 145
    },
    {
      "epoch": 6.818181818181818,
      "grad_norm": 0.5144734382629395,
      "learning_rate": 0.008425287356321838,
      "loss": 2.1883,
      "step": 150
    },
    {
      "epoch": 7.045454545454546,
      "grad_norm": 0.3955673277378082,
      "learning_rate": 0.008367816091954024,
      "loss": 2.1261,
      "step": 155
    },
    {
      "epoch": 7.2727272727272725,
      "grad_norm": 0.47012946009635925,
      "learning_rate": 0.008310344827586207,
      "loss": 2.0017,
      "step": 160
    },
    {
      "epoch": 7.5,
      "grad_norm": 0.45242980122566223,
      "learning_rate": 0.008252873563218391,
      "loss": 2.2401,
      "step": 165
    },
    {
      "epoch": 7.7272727272727275,
      "grad_norm": 0.4938039183616638,
      "learning_rate": 0.008195402298850575,
      "loss": 1.9854,
      "step": 170
    },
    {
      "epoch": 7.954545454545455,
      "grad_norm": 0.4803965091705322,
      "learning_rate": 0.008137931034482758,
      "loss": 2.1239,
      "step": 175
    },
    {
      "epoch": 8.181818181818182,
      "grad_norm": 0.45792096853256226,
      "learning_rate": 0.008080459770114942,
      "loss": 2.0494,
      "step": 180
    },
    {
      "epoch": 8.409090909090908,
      "grad_norm": 0.7061192989349365,
      "learning_rate": 0.008022988505747126,
      "loss": 2.0598,
      "step": 185
    },
    {
      "epoch": 8.636363636363637,
      "grad_norm": 0.5679919719696045,
      "learning_rate": 0.00796551724137931,
      "loss": 2.2285,
      "step": 190
    },
    {
      "epoch": 8.863636363636363,
      "grad_norm": 0.51705402135849,
      "learning_rate": 0.007908045977011495,
      "loss": 1.9956,
      "step": 195
    },
    {
      "epoch": 9.090909090909092,
      "grad_norm": 0.545191764831543,
      "learning_rate": 0.00785057471264368,
      "loss": 2.0427,
      "step": 200
    },
    {
      "epoch": 9.318181818181818,
      "grad_norm": 0.9836992621421814,
      "learning_rate": 0.007793103448275863,
      "loss": 1.9836,
      "step": 205
    },
    {
      "epoch": 9.545454545454545,
      "grad_norm": 0.4916435480117798,
      "learning_rate": 0.007735632183908046,
      "loss": 1.9055,
      "step": 210
    },
    {
      "epoch": 9.772727272727273,
      "grad_norm": 0.7231175899505615,
      "learning_rate": 0.00767816091954023,
      "loss": 1.9007,
      "step": 215
    },
    {
      "epoch": 10.0,
      "grad_norm": 0.6536340713500977,
      "learning_rate": 0.007620689655172414,
      "loss": 2.1322,
      "step": 220
    },
    {
      "epoch": 10.227272727272727,
      "grad_norm": 0.47098514437675476,
      "learning_rate": 0.007563218390804598,
      "loss": 2.1346,
      "step": 225
    },
    {
      "epoch": 10.454545454545455,
      "grad_norm": 0.49506503343582153,
      "learning_rate": 0.007505747126436781,
      "loss": 1.8369,
      "step": 230
    },
    {
      "epoch": 10.681818181818182,
      "grad_norm": 0.4904889166355133,
      "learning_rate": 0.007448275862068966,
      "loss": 1.8111,
      "step": 235
    },
    {
      "epoch": 10.909090909090908,
      "grad_norm": 0.4797939658164978,
      "learning_rate": 0.00739080459770115,
      "loss": 2.11,
      "step": 240
    },
    {
      "epoch": 11.136363636363637,
      "grad_norm": 0.567205011844635,
      "learning_rate": 0.007333333333333333,
      "loss": 2.0027,
      "step": 245
    },
    {
      "epoch": 11.363636363636363,
      "grad_norm": 0.44183704257011414,
      "learning_rate": 0.0072758620689655175,
      "loss": 1.9013,
      "step": 250
    },
    {
      "epoch": 11.590909090909092,
      "grad_norm": 0.5096936821937561,
      "learning_rate": 0.007218390804597701,
      "loss": 1.7215,
      "step": 255
    },
    {
      "epoch": 11.818181818181818,
      "grad_norm": 0.45881953835487366,
      "learning_rate": 0.007160919540229885,
      "loss": 1.9548,
      "step": 260
    },
    {
      "epoch": 12.045454545454545,
      "grad_norm": 0.47226226329803467,
      "learning_rate": 0.0071034482758620685,
      "loss": 1.8454,
      "step": 265
    },
    {
      "epoch": 12.272727272727273,
      "grad_norm": 0.5118619203567505,
      "learning_rate": 0.007045977011494254,
      "loss": 1.7094,
      "step": 270
    },
    {
      "epoch": 12.5,
      "grad_norm": 0.47077083587646484,
      "learning_rate": 0.006988505747126437,
      "loss": 1.952,
      "step": 275
    },
    {
      "epoch": 12.727272727272727,
      "grad_norm": 0.45885810256004333,
      "learning_rate": 0.006931034482758621,
      "loss": 1.7017,
      "step": 280
    },
    {
      "epoch": 12.954545454545455,
      "grad_norm": 0.44890812039375305,
      "learning_rate": 0.006873563218390805,
      "loss": 2.0258,
      "step": 285
    },
    {
      "epoch": 13.181818181818182,
      "grad_norm": 0.44398990273475647,
      "learning_rate": 0.006816091954022988,
      "loss": 1.791,
      "step": 290
    },
    {
      "epoch": 13.409090909090908,
      "grad_norm": 0.4875212609767914,
      "learning_rate": 0.006758620689655173,
      "loss": 1.784,
      "step": 295
    },
    {
      "epoch": 13.636363636363637,
      "grad_norm": 0.42783740162849426,
      "learning_rate": 0.0067011494252873565,
      "loss": 1.9402,
      "step": 300
    },
    {
      "epoch": 13.863636363636363,
      "grad_norm": 0.431281715631485,
      "learning_rate": 0.006643678160919541,
      "loss": 1.7288,
      "step": 305
    },
    {
      "epoch": 14.090909090909092,
      "grad_norm": 0.4261733889579773,
      "learning_rate": 0.006586206896551724,
      "loss": 1.7148,
      "step": 310
    },
    {
      "epoch": 14.318181818181818,
      "grad_norm": 0.5008634924888611,
      "learning_rate": 0.0065287356321839084,
      "loss": 1.7041,
      "step": 315
    },
    {
      "epoch": 14.545454545454545,
      "grad_norm": 0.4829307496547699,
      "learning_rate": 0.006471264367816092,
      "loss": 1.653,
      "step": 320
    },
    {
      "epoch": 14.772727272727273,
      "grad_norm": 0.5645098090171814,
      "learning_rate": 0.006413793103448275,
      "loss": 1.7629,
      "step": 325
    },
    {
      "epoch": 15.0,
      "grad_norm": 0.5721149444580078,
      "learning_rate": 0.00635632183908046,
      "loss": 1.9465,
      "step": 330
    },
    {
      "epoch": 15.227272727272727,
      "grad_norm": 0.4908706545829773,
      "learning_rate": 0.006298850574712644,
      "loss": 1.5398,
      "step": 335
    },
    {
      "epoch": 15.454545454545455,
      "grad_norm": 0.4246895909309387,
      "learning_rate": 0.006241379310344828,
      "loss": 1.719,
      "step": 340
    },
    {
      "epoch": 15.681818181818182,
      "grad_norm": 0.4181322753429413,
      "learning_rate": 0.006183908045977011,
      "loss": 1.8752,
      "step": 345
    },
    {
      "epoch": 15.909090909090908,
      "grad_norm": 0.41400495171546936,
      "learning_rate": 0.006126436781609196,
      "loss": 1.6326,
      "step": 350
    },
    {
      "epoch": 16.136363636363637,
      "grad_norm": 0.444202184677124,
      "learning_rate": 0.006068965517241379,
      "loss": 1.7654,
      "step": 355
    },
    {
      "epoch": 16.363636363636363,
      "grad_norm": 0.48888641595840454,
      "learning_rate": 0.006011494252873564,
      "loss": 1.496,
      "step": 360
    },
    {
      "epoch": 16.59090909090909,
      "grad_norm": 0.47786587476730347,
      "learning_rate": 0.0059540229885057475,
      "loss": 1.6006,
      "step": 365
    },
    {
      "epoch": 16.818181818181817,
      "grad_norm": 0.7497329115867615,
      "learning_rate": 0.005896551724137931,
      "loss": 1.8875,
      "step": 370
    },
    {
      "epoch": 17.045454545454547,
      "grad_norm": 0.4638390839099884,
      "learning_rate": 0.005839080459770115,
      "loss": 1.796,
      "step": 375
    },
    {
      "epoch": 17.272727272727273,
      "grad_norm": 0.48922452330589294,
      "learning_rate": 0.0057816091954022986,
      "loss": 1.5711,
      "step": 380
    },
    {
      "epoch": 17.5,
      "grad_norm": 0.4247593581676483,
      "learning_rate": 0.005724137931034483,
      "loss": 1.5775,
      "step": 385
    },
    {
      "epoch": 17.727272727272727,
      "grad_norm": 0.4840433895587921,
      "learning_rate": 0.005666666666666666,
      "loss": 1.4901,
      "step": 390
    },
    {
      "epoch": 17.954545454545453,
      "grad_norm": 0.43046608567237854,
      "learning_rate": 0.005609195402298851,
      "loss": 1.6943,
      "step": 395
    },
    {
      "epoch": 18.181818181818183,
      "grad_norm": 0.4364667236804962,
      "learning_rate": 0.005551724137931035,
      "loss": 1.7277,
      "step": 400
    },
    {
      "epoch": 18.40909090909091,
      "grad_norm": 0.5152137279510498,
      "learning_rate": 0.005494252873563218,
      "loss": 1.6264,
      "step": 405
    },
    {
      "epoch": 18.636363636363637,
      "grad_norm": 0.46983444690704346,
      "learning_rate": 0.005436781609195402,
      "loss": 1.5875,
      "step": 410
    },
    {
      "epoch": 18.863636363636363,
      "grad_norm": 0.522465169429779,
      "learning_rate": 0.005379310344827586,
      "loss": 1.5645,
      "step": 415
    },
    {
      "epoch": 19.09090909090909,
      "grad_norm": 0.5005538463592529,
      "learning_rate": 0.005321839080459771,
      "loss": 1.6636,
      "step": 420
    },
    {
      "epoch": 19.318181818181817,
      "grad_norm": 0.47570329904556274,
      "learning_rate": 0.005264367816091954,
      "loss": 1.7014,
      "step": 425
    },
    {
      "epoch": 19.545454545454547,
      "grad_norm": 0.6415995955467224,
      "learning_rate": 0.0052068965517241385,
      "loss": 1.691,
      "step": 430
    },
    {
      "epoch": 19.772727272727273,
      "grad_norm": 0.4490019977092743,
      "learning_rate": 0.005149425287356322,
      "loss": 1.4301,
      "step": 435
    },
    {
      "epoch": 20.0,
      "grad_norm": 0.6002772450447083,
      "learning_rate": 0.005091954022988506,
      "loss": 1.3633,
      "step": 440
    },
    {
      "epoch": 20.227272727272727,
      "grad_norm": 0.5132799744606018,
      "learning_rate": 0.0050344827586206896,
      "loss": 1.4293,
      "step": 445
    },
    {
      "epoch": 20.454545454545453,
      "grad_norm": 0.4430108964443207,
      "learning_rate": 0.004977011494252874,
      "loss": 1.6726,
      "step": 450
    },
    {
      "epoch": 20.681818181818183,
      "grad_norm": 0.43500861525535583,
      "learning_rate": 0.004919540229885058,
      "loss": 1.3842,
      "step": 455
    },
    {
      "epoch": 20.90909090909091,
      "grad_norm": 0.43241772055625916,
      "learning_rate": 0.0048620689655172415,
      "loss": 1.6516,
      "step": 460
    },
    {
      "epoch": 21.136363636363637,
      "grad_norm": 0.468680739402771,
      "learning_rate": 0.004804597701149425,
      "loss": 1.527,
      "step": 465
    },
    {
      "epoch": 21.363636363636363,
      "grad_norm": 0.47128552198410034,
      "learning_rate": 0.004747126436781609,
      "loss": 1.6352,
      "step": 470
    },
    {
      "epoch": 21.59090909090909,
      "grad_norm": 0.4606361389160156,
      "learning_rate": 0.004689655172413793,
      "loss": 1.3338,
      "step": 475
    },
    {
      "epoch": 21.818181818181817,
      "grad_norm": 0.4553014934062958,
      "learning_rate": 0.004632183908045977,
      "loss": 1.4516,
      "step": 480
    },
    {
      "epoch": 22.045454545454547,
      "grad_norm": 0.4452090263366699,
      "learning_rate": 0.004574712643678161,
      "loss": 1.5591,
      "step": 485
    },
    {
      "epoch": 22.272727272727273,
      "grad_norm": 0.4436010420322418,
      "learning_rate": 0.004517241379310345,
      "loss": 1.4382,
      "step": 490
    },
    {
      "epoch": 22.5,
      "grad_norm": 0.4175149202346802,
      "learning_rate": 0.004459770114942529,
      "loss": 1.4674,
      "step": 495
    },
    {
      "epoch": 22.727272727272727,
      "grad_norm": 0.48347383737564087,
      "learning_rate": 0.004402298850574713,
      "loss": 1.3967,
      "step": 500
    },
    {
      "epoch": 22.954545454545453,
      "grad_norm": 0.4919760525226593,
      "learning_rate": 0.004344827586206896,
      "loss": 1.4261,
      "step": 505
    },
    {
      "epoch": 23.181818181818183,
      "grad_norm": 0.625944972038269,
      "learning_rate": 0.0042873563218390805,
      "loss": 1.5386,
      "step": 510
    },
    {
      "epoch": 23.40909090909091,
      "grad_norm": 0.5413931608200073,
      "learning_rate": 0.004229885057471265,
      "loss": 1.3076,
      "step": 515
    },
    {
      "epoch": 23.636363636363637,
      "grad_norm": 0.4260912537574768,
      "learning_rate": 0.004172413793103448,
      "loss": 1.5539,
      "step": 520
    },
    {
      "epoch": 23.863636363636363,
      "grad_norm": 0.48671630024909973,
      "learning_rate": 0.0041149425287356324,
      "loss": 1.5153,
      "step": 525
    },
    {
      "epoch": 24.09090909090909,
      "grad_norm": 0.44753003120422363,
      "learning_rate": 0.004057471264367817,
      "loss": 1.4158,
      "step": 530
    },
    {
      "epoch": 24.318181818181817,
      "grad_norm": 0.476815402507782,
      "learning_rate": 0.004,
      "loss": 1.6136,
      "step": 535
    },
    {
      "epoch": 24.545454545454547,
      "grad_norm": 0.4353930354118347,
      "learning_rate": 0.0039425287356321835,
      "loss": 1.3683,
      "step": 540
    },
    {
      "epoch": 24.772727272727273,
      "grad_norm": 0.46039459109306335,
      "learning_rate": 0.0038850574712643677,
      "loss": 1.3037,
      "step": 545
    },
    {
      "epoch": 25.0,
      "grad_norm": 0.5690804719924927,
      "learning_rate": 0.0038275862068965515,
      "loss": 1.2729,
      "step": 550
    },
    {
      "epoch": 25.227272727272727,
      "grad_norm": 0.4818664491176605,
      "learning_rate": 0.003770114942528736,
      "loss": 1.2259,
      "step": 555
    },
    {
      "epoch": 25.454545454545453,
      "grad_norm": 0.7860492467880249,
      "learning_rate": 0.0037126436781609196,
      "loss": 1.2216,
      "step": 560
    },
    {
      "epoch": 25.681818181818183,
      "grad_norm": 0.4448511302471161,
      "learning_rate": 0.0036551724137931034,
      "loss": 1.4129,
      "step": 565
    },
    {
      "epoch": 25.90909090909091,
      "grad_norm": 0.7232092618942261,
      "learning_rate": 0.0035977011494252877,
      "loss": 1.5227,
      "step": 570
    },
    {
      "epoch": 26.136363636363637,
      "grad_norm": 0.43298178911209106,
      "learning_rate": 0.0035402298850574715,
      "loss": 1.6421,
      "step": 575
    },
    {
      "epoch": 26.363636363636363,
      "grad_norm": 0.5449762344360352,
      "learning_rate": 0.003482758620689655,
      "loss": 1.2511,
      "step": 580
    },
    {
      "epoch": 26.59090909090909,
      "grad_norm": 0.4772466719150543,
      "learning_rate": 0.003425287356321839,
      "loss": 1.3205,
      "step": 585
    },
    {
      "epoch": 26.818181818181817,
      "grad_norm": 0.6043494939804077,
      "learning_rate": 0.003367816091954023,
      "loss": 1.4764,
      "step": 590
    },
    {
      "epoch": 27.045454545454547,
      "grad_norm": 0.5146710276603699,
      "learning_rate": 0.003310344827586207,
      "loss": 1.1525,
      "step": 595
    },
    {
      "epoch": 27.272727272727273,
      "grad_norm": 0.45481276512145996,
      "learning_rate": 0.003252873563218391,
      "loss": 1.1933,
      "step": 600
    },
    {
      "epoch": 27.5,
      "grad_norm": 0.45183947682380676,
      "learning_rate": 0.003195402298850575,
      "loss": 1.3611,
      "step": 605
    },
    {
      "epoch": 27.727272727272727,
      "grad_norm": 0.5204657912254333,
      "learning_rate": 0.0031379310344827587,
      "loss": 1.306,
      "step": 610
    },
    {
      "epoch": 27.954545454545453,
      "grad_norm": 0.5054537057876587,
      "learning_rate": 0.003080459770114943,
      "loss": 1.4353,
      "step": 615
    },
    {
      "epoch": 28.181818181818183,
      "grad_norm": 0.43956658244132996,
      "learning_rate": 0.0030229885057471264,
      "loss": 1.328,
      "step": 620
    },
    {
      "epoch": 28.40909090909091,
      "grad_norm": 0.6516703963279724,
      "learning_rate": 0.00296551724137931,
      "loss": 1.3566,
      "step": 625
    },
    {
      "epoch": 28.636363636363637,
      "grad_norm": 0.4461202919483185,
      "learning_rate": 0.002908045977011494,
      "loss": 1.3861,
      "step": 630
    },
    {
      "epoch": 28.863636363636363,
      "grad_norm": 0.48382917046546936,
      "learning_rate": 0.0028505747126436783,
      "loss": 1.2976,
      "step": 635
    },
    {
      "epoch": 29.09090909090909,
      "grad_norm": 0.6438767910003662,
      "learning_rate": 0.002793103448275862,
      "loss": 1.213,
      "step": 640
    },
    {
      "epoch": 29.318181818181817,
      "grad_norm": 0.4616726040840149,
      "learning_rate": 0.0027356321839080463,
      "loss": 1.3195,
      "step": 645
    },
    {
      "epoch": 29.545454545454547,
      "grad_norm": 0.4897880256175995,
      "learning_rate": 0.00267816091954023,
      "loss": 1.1068,
      "step": 650
    },
    {
      "epoch": 29.772727272727273,
      "grad_norm": 0.4638940095901489,
      "learning_rate": 0.002620689655172414,
      "loss": 1.4471,
      "step": 655
    },
    {
      "epoch": 30.0,
      "grad_norm": 0.49922090768814087,
      "learning_rate": 0.0025632183908045974,
      "loss": 1.2148,
      "step": 660
    },
    {
      "epoch": 30.227272727272727,
      "grad_norm": 0.38181063532829285,
      "learning_rate": 0.0025057471264367816,
      "loss": 1.1576,
      "step": 665
    },
    {
      "epoch": 30.454545454545453,
      "grad_norm": 0.5602654218673706,
      "learning_rate": 0.0024482758620689654,
      "loss": 1.367,
      "step": 670
    },
    {
      "epoch": 30.681818181818183,
      "grad_norm": 0.4325083792209625,
      "learning_rate": 0.0023908045977011493,
      "loss": 1.228,
      "step": 675
    },
    {
      "epoch": 30.90909090909091,
      "grad_norm": 0.44154396653175354,
      "learning_rate": 0.0023333333333333335,
      "loss": 1.2493,
      "step": 680
    },
    {
      "epoch": 31.136363636363637,
      "grad_norm": 0.44649308919906616,
      "learning_rate": 0.0022758620689655173,
      "loss": 1.1589,
      "step": 685
    },
    {
      "epoch": 31.363636363636363,
      "grad_norm": 0.4343907833099365,
      "learning_rate": 0.002218390804597701,
      "loss": 1.2502,
      "step": 690
    },
    {
      "epoch": 31.59090909090909,
      "grad_norm": 0.5046719312667847,
      "learning_rate": 0.002160919540229885,
      "loss": 1.1493,
      "step": 695
    },
    {
      "epoch": 31.818181818181817,
      "grad_norm": 0.4013687074184418,
      "learning_rate": 0.0021034482758620692,
      "loss": 1.3034,
      "step": 700
    },
    {
      "epoch": 32.04545454545455,
      "grad_norm": 0.405745267868042,
      "learning_rate": 0.002045977011494253,
      "loss": 1.3118,
      "step": 705
    },
    {
      "epoch": 32.27272727272727,
      "grad_norm": 0.4506112039089203,
      "learning_rate": 0.001988505747126437,
      "loss": 1.0649,
      "step": 710
    },
    {
      "epoch": 32.5,
      "grad_norm": 0.46546080708503723,
      "learning_rate": 0.0019310344827586207,
      "loss": 1.1216,
      "step": 715
    },
    {
      "epoch": 32.72727272727273,
      "grad_norm": 0.5887216925621033,
      "learning_rate": 0.0018735632183908048,
      "loss": 1.3153,
      "step": 720
    },
    {
      "epoch": 32.95454545454545,
      "grad_norm": 0.48805519938468933,
      "learning_rate": 0.0018160919540229886,
      "loss": 1.3287,
      "step": 725
    },
    {
      "epoch": 33.18181818181818,
      "grad_norm": 0.4737352430820465,
      "learning_rate": 0.0017586206896551724,
      "loss": 1.273,
      "step": 730
    },
    {
      "epoch": 33.40909090909091,
      "grad_norm": 0.5016063451766968,
      "learning_rate": 0.0017011494252873564,
      "loss": 1.1709,
      "step": 735
    },
    {
      "epoch": 33.63636363636363,
      "grad_norm": 0.4072301685810089,
      "learning_rate": 0.0016436781609195403,
      "loss": 0.9653,
      "step": 740
    },
    {
      "epoch": 33.86363636363637,
      "grad_norm": 0.4535064101219177,
      "learning_rate": 0.0015862068965517243,
      "loss": 1.2098,
      "step": 745
    },
    {
      "epoch": 34.09090909090909,
      "grad_norm": 0.5220146775245667,
      "learning_rate": 0.001528735632183908,
      "loss": 1.4787,
      "step": 750
    },
    {
      "epoch": 34.31818181818182,
      "grad_norm": 0.4724150598049164,
      "learning_rate": 0.001471264367816092,
      "loss": 1.1126,
      "step": 755
    },
    {
      "epoch": 34.54545454545455,
      "grad_norm": 0.4751241207122803,
      "learning_rate": 0.001413793103448276,
      "loss": 1.3914,
      "step": 760
    },
    {
      "epoch": 34.77272727272727,
      "grad_norm": 0.41146814823150635,
      "learning_rate": 0.00135632183908046,
      "loss": 1.0677,
      "step": 765
    },
    {
      "epoch": 35.0,
      "grad_norm": 0.5637047290802002,
      "learning_rate": 0.0012988505747126436,
      "loss": 1.0206,
      "step": 770
    },
    {
      "epoch": 35.22727272727273,
      "grad_norm": 0.4318408668041229,
      "learning_rate": 0.0012413793103448277,
      "loss": 1.1888,
      "step": 775
    },
    {
      "epoch": 35.45454545454545,
      "grad_norm": 0.4501388669013977,
      "learning_rate": 0.0011839080459770115,
      "loss": 1.1123,
      "step": 780
    },
    {
      "epoch": 35.68181818181818,
      "grad_norm": 0.45081910490989685,
      "learning_rate": 0.0011264367816091953,
      "loss": 1.1707,
      "step": 785
    },
    {
      "epoch": 35.90909090909091,
      "grad_norm": 0.4143672287464142,
      "learning_rate": 0.0010689655172413793,
      "loss": 1.2488,
      "step": 790
    },
    {
      "epoch": 36.13636363636363,
      "grad_norm": 0.46762481331825256,
      "learning_rate": 0.0010114942528735632,
      "loss": 1.0244,
      "step": 795
    },
    {
      "epoch": 36.36363636363637,
      "grad_norm": 0.463997483253479,
      "learning_rate": 0.0009540229885057472,
      "loss": 1.0465,
      "step": 800
    },
    {
      "epoch": 36.59090909090909,
      "grad_norm": 0.4561272859573364,
      "learning_rate": 0.000896551724137931,
      "loss": 1.1317,
      "step": 805
    },
    {
      "epoch": 36.81818181818182,
      "grad_norm": 0.4818389415740967,
      "learning_rate": 0.0008390804597701151,
      "loss": 1.181,
      "step": 810
    },
    {
      "epoch": 37.04545454545455,
      "grad_norm": 0.438213586807251,
      "learning_rate": 0.0007816091954022989,
      "loss": 1.243,
      "step": 815
    },
    {
      "epoch": 37.27272727272727,
      "grad_norm": 0.46957212686538696,
      "learning_rate": 0.0007241379310344828,
      "loss": 1.0651,
      "step": 820
    },
    {
      "epoch": 37.5,
      "grad_norm": 0.48171666264533997,
      "learning_rate": 0.0006666666666666666,
      "loss": 1.0851,
      "step": 825
    },
    {
      "epoch": 37.72727272727273,
      "grad_norm": 0.4543040692806244,
      "learning_rate": 0.0006091954022988506,
      "loss": 1.1667,
      "step": 830
    },
    {
      "epoch": 37.95454545454545,
      "grad_norm": 0.48580944538116455,
      "learning_rate": 0.0005517241379310345,
      "loss": 1.3083,
      "step": 835
    },
    {
      "epoch": 38.18181818181818,
      "grad_norm": 0.43165668845176697,
      "learning_rate": 0.0004942528735632184,
      "loss": 1.1345,
      "step": 840
    },
    {
      "epoch": 38.40909090909091,
      "grad_norm": 2.5148165225982666,
      "learning_rate": 0.0004367816091954023,
      "loss": 1.0885,
      "step": 845
    },
    {
      "epoch": 38.63636363636363,
      "grad_norm": 0.42568275332450867,
      "learning_rate": 0.0003793103448275862,
      "loss": 1.0999,
      "step": 850
    },
    {
      "epoch": 38.86363636363637,
      "grad_norm": 0.4557811915874481,
      "learning_rate": 0.0003218390804597701,
      "loss": 1.1466,
      "step": 855
    },
    {
      "epoch": 39.09090909090909,
      "grad_norm": 0.41968369483947754,
      "learning_rate": 0.00026436781609195405,
      "loss": 1.0255,
      "step": 860
    },
    {
      "epoch": 39.31818181818182,
      "grad_norm": 0.4746061861515045,
      "learning_rate": 0.00020689655172413793,
      "loss": 1.0805,
      "step": 865
    },
    {
      "epoch": 39.54545454545455,
      "grad_norm": 0.4709218442440033,
      "learning_rate": 0.00014942528735632183,
      "loss": 1.1761,
      "step": 870
    },
    {
      "epoch": 39.77272727272727,
      "grad_norm": 0.46575096249580383,
      "learning_rate": 9.195402298850575e-05,
      "loss": 1.2098,
      "step": 875
    },
    {
      "epoch": 40.0,
      "grad_norm": 0.5051140785217285,
      "learning_rate": 3.4482758620689657e-05,
      "loss": 1.0868,
      "step": 880
    }
  ],
  "logging_steps": 5,
  "max_steps": 880,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 40,
  "save_steps": 500,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": true
      },
      "attributes": {}
    }
  },
  "total_flos": 954590035968000.0,
  "train_batch_size": 4,
  "trial_name": null,
  "trial_params": null
}
