{
  "best_global_step": null,
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 60.0,
  "eval_steps": 500,
  "global_step": 1320,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.22727272727272727,
      "grad_norm": 0.14406989514827728,
      "learning_rate": 0.0003,
      "loss": 3.8355,
      "step": 5
    },
    {
      "epoch": 0.45454545454545453,
      "grad_norm": 0.24180914461612701,
      "learning_rate": 0.0007,
      "loss": 3.7616,
      "step": 10
    },
    {
      "epoch": 0.6818181818181818,
      "grad_norm": 0.31816229224205017,
      "learning_rate": 0.0009984732824427482,
      "loss": 3.842,
      "step": 15
    },
    {
      "epoch": 0.9090909090909091,
      "grad_norm": 0.4208875298500061,
      "learning_rate": 0.0009946564885496183,
      "loss": 3.347,
      "step": 20
    },
    {
      "epoch": 1.1363636363636362,
      "grad_norm": 0.49439647793769836,
      "learning_rate": 0.0009908396946564887,
      "loss": 3.1073,
      "step": 25
    },
    {
      "epoch": 1.3636363636363638,
      "grad_norm": 0.6462622284889221,
      "learning_rate": 0.0009870229007633588,
      "loss": 3.3025,
      "step": 30
    },
    {
      "epoch": 1.5909090909090908,
      "grad_norm": 0.700653076171875,
      "learning_rate": 0.000983206106870229,
      "loss": 2.8621,
      "step": 35
    },
    {
      "epoch": 1.8181818181818183,
      "grad_norm": 0.5578944683074951,
      "learning_rate": 0.0009793893129770993,
      "loss": 3.0078,
      "step": 40
    },
    {
      "epoch": 2.0454545454545454,
      "grad_norm": 0.6582105755805969,
      "learning_rate": 0.0009755725190839695,
      "loss": 3.0008,
      "step": 45
    },
    {
      "epoch": 2.2727272727272725,
      "grad_norm": 0.6888976693153381,
      "learning_rate": 0.0009717557251908398,
      "loss": 2.8127,
      "step": 50
    },
    {
      "epoch": 2.5,
      "grad_norm": 0.7491772174835205,
      "learning_rate": 0.0009687022900763359,
      "loss": 2.7623,
      "step": 55
    },
    {
      "epoch": 2.7272727272727275,
      "grad_norm": 0.6898749470710754,
      "learning_rate": 0.0009648854961832061,
      "loss": 2.6855,
      "step": 60
    },
    {
      "epoch": 2.9545454545454546,
      "grad_norm": 0.7843835949897766,
      "learning_rate": 0.0009610687022900764,
      "loss": 2.8007,
      "step": 65
    },
    {
      "epoch": 3.1818181818181817,
      "grad_norm": 0.7692344188690186,
      "learning_rate": 0.0009572519083969465,
      "loss": 2.8062,
      "step": 70
    },
    {
      "epoch": 3.409090909090909,
      "grad_norm": 0.6713203191757202,
      "learning_rate": 0.0009534351145038168,
      "loss": 2.6276,
      "step": 75
    },
    {
      "epoch": 3.6363636363636362,
      "grad_norm": 0.7902217507362366,
      "learning_rate": 0.0009496183206106871,
      "loss": 2.7252,
      "step": 80
    },
    {
      "epoch": 3.8636363636363638,
      "grad_norm": 0.6736592054367065,
      "learning_rate": 0.0009458015267175573,
      "loss": 2.7222,
      "step": 85
    },
    {
      "epoch": 4.090909090909091,
      "grad_norm": 0.7736155986785889,
      "learning_rate": 0.0009419847328244275,
      "loss": 2.9273,
      "step": 90
    },
    {
      "epoch": 4.318181818181818,
      "grad_norm": 0.7593094706535339,
      "learning_rate": 0.0009381679389312977,
      "loss": 2.4695,
      "step": 95
    },
    {
      "epoch": 4.545454545454545,
      "grad_norm": 0.8795844912528992,
      "learning_rate": 0.0009343511450381679,
      "loss": 2.7844,
      "step": 100
    },
    {
      "epoch": 4.7727272727272725,
      "grad_norm": 0.8215889930725098,
      "learning_rate": 0.0009305343511450382,
      "loss": 2.6351,
      "step": 105
    },
    {
      "epoch": 5.0,
      "grad_norm": 0.9932531118392944,
      "learning_rate": 0.0009267175572519084,
      "loss": 2.5146,
      "step": 110
    },
    {
      "epoch": 5.2272727272727275,
      "grad_norm": 0.9456309676170349,
      "learning_rate": 0.0009229007633587786,
      "loss": 2.4981,
      "step": 115
    },
    {
      "epoch": 5.454545454545454,
      "grad_norm": 1.0449953079223633,
      "learning_rate": 0.0009190839694656488,
      "loss": 2.502,
      "step": 120
    },
    {
      "epoch": 5.681818181818182,
      "grad_norm": 1.0122096538543701,
      "learning_rate": 0.0009152671755725192,
      "loss": 2.5437,
      "step": 125
    },
    {
      "epoch": 5.909090909090909,
      "grad_norm": 1.0497939586639404,
      "learning_rate": 0.0009114503816793893,
      "loss": 2.5071,
      "step": 130
    },
    {
      "epoch": 6.136363636363637,
      "grad_norm": 0.8380274772644043,
      "learning_rate": 0.0009076335877862596,
      "loss": 2.6571,
      "step": 135
    },
    {
      "epoch": 6.363636363636363,
      "grad_norm": 0.8041980266571045,
      "learning_rate": 0.0009038167938931297,
      "loss": 2.4762,
      "step": 140
    },
    {
      "epoch": 6.590909090909091,
      "grad_norm": 0.9010542631149292,
      "learning_rate": 0.0009000000000000001,
      "loss": 2.4706,
      "step": 145
    },
    {
      "epoch": 6.818181818181818,
      "grad_norm": 0.9289631247520447,
      "learning_rate": 0.0008961832061068702,
      "loss": 2.4805,
      "step": 150
    },
    {
      "epoch": 7.045454545454546,
      "grad_norm": 0.9050604104995728,
      "learning_rate": 0.0008923664122137405,
      "loss": 2.5047,
      "step": 155
    },
    {
      "epoch": 7.2727272727272725,
      "grad_norm": 0.9439448118209839,
      "learning_rate": 0.0008885496183206106,
      "loss": 2.3616,
      "step": 160
    },
    {
      "epoch": 7.5,
      "grad_norm": 0.903203010559082,
      "learning_rate": 0.000884732824427481,
      "loss": 2.3924,
      "step": 165
    },
    {
      "epoch": 7.7272727272727275,
      "grad_norm": 0.9345096349716187,
      "learning_rate": 0.0008809160305343512,
      "loss": 2.5058,
      "step": 170
    },
    {
      "epoch": 7.954545454545455,
      "grad_norm": 1.0921214818954468,
      "learning_rate": 0.0008770992366412214,
      "loss": 2.331,
      "step": 175
    },
    {
      "epoch": 8.181818181818182,
      "grad_norm": 0.8690490126609802,
      "learning_rate": 0.0008732824427480916,
      "loss": 2.1189,
      "step": 180
    },
    {
      "epoch": 8.409090909090908,
      "grad_norm": 1.1409980058670044,
      "learning_rate": 0.0008694656488549618,
      "loss": 2.4545,
      "step": 185
    },
    {
      "epoch": 8.636363636363637,
      "grad_norm": 0.976198136806488,
      "learning_rate": 0.0008656488549618321,
      "loss": 2.4533,
      "step": 190
    },
    {
      "epoch": 8.863636363636363,
      "grad_norm": 0.9370555281639099,
      "learning_rate": 0.0008618320610687023,
      "loss": 2.3896,
      "step": 195
    },
    {
      "epoch": 9.090909090909092,
      "grad_norm": 0.8967379927635193,
      "learning_rate": 0.0008580152671755725,
      "loss": 2.2458,
      "step": 200
    },
    {
      "epoch": 9.318181818181818,
      "grad_norm": 1.1327173709869385,
      "learning_rate": 0.0008541984732824427,
      "loss": 2.3148,
      "step": 205
    },
    {
      "epoch": 9.545454545454545,
      "grad_norm": 1.031205415725708,
      "learning_rate": 0.000850381679389313,
      "loss": 2.2347,
      "step": 210
    },
    {
      "epoch": 9.772727272727273,
      "grad_norm": 0.9874230623245239,
      "learning_rate": 0.0008465648854961833,
      "loss": 2.271,
      "step": 215
    },
    {
      "epoch": 10.0,
      "grad_norm": 1.3869073390960693,
      "learning_rate": 0.0008427480916030534,
      "loss": 2.3464,
      "step": 220
    },
    {
      "epoch": 10.227272727272727,
      "grad_norm": 1.018091082572937,
      "learning_rate": 0.0008389312977099237,
      "loss": 2.1849,
      "step": 225
    },
    {
      "epoch": 10.454545454545455,
      "grad_norm": 1.0573043823242188,
      "learning_rate": 0.0008351145038167939,
      "loss": 2.2971,
      "step": 230
    },
    {
      "epoch": 10.681818181818182,
      "grad_norm": 1.0759614706039429,
      "learning_rate": 0.0008312977099236642,
      "loss": 2.2433,
      "step": 235
    },
    {
      "epoch": 10.909090909090908,
      "grad_norm": 1.2005183696746826,
      "learning_rate": 0.0008274809160305343,
      "loss": 2.2685,
      "step": 240
    },
    {
      "epoch": 11.136363636363637,
      "grad_norm": 1.1572335958480835,
      "learning_rate": 0.0008236641221374046,
      "loss": 2.2392,
      "step": 245
    },
    {
      "epoch": 11.363636363636363,
      "grad_norm": 1.5673490762710571,
      "learning_rate": 0.0008198473282442749,
      "loss": 2.1467,
      "step": 250
    },
    {
      "epoch": 11.590909090909092,
      "grad_norm": 1.1458569765090942,
      "learning_rate": 0.0008160305343511451,
      "loss": 2.2063,
      "step": 255
    },
    {
      "epoch": 11.818181818181818,
      "grad_norm": 1.0050387382507324,
      "learning_rate": 0.0008122137404580153,
      "loss": 2.164,
      "step": 260
    },
    {
      "epoch": 12.045454545454545,
      "grad_norm": 1.2586658000946045,
      "learning_rate": 0.0008083969465648855,
      "loss": 2.1981,
      "step": 265
    },
    {
      "epoch": 12.272727272727273,
      "grad_norm": 1.1507811546325684,
      "learning_rate": 0.0008045801526717558,
      "loss": 2.2947,
      "step": 270
    },
    {
      "epoch": 12.5,
      "grad_norm": 1.050046682357788,
      "learning_rate": 0.000800763358778626,
      "loss": 2.1187,
      "step": 275
    },
    {
      "epoch": 12.727272727272727,
      "grad_norm": 1.1648669242858887,
      "learning_rate": 0.0007969465648854962,
      "loss": 2.1861,
      "step": 280
    },
    {
      "epoch": 12.954545454545455,
      "grad_norm": 1.263203740119934,
      "learning_rate": 0.0007931297709923664,
      "loss": 2.1025,
      "step": 285
    },
    {
      "epoch": 13.181818181818182,
      "grad_norm": 1.2264299392700195,
      "learning_rate": 0.0007893129770992366,
      "loss": 2.1971,
      "step": 290
    },
    {
      "epoch": 13.409090909090908,
      "grad_norm": 1.5942937135696411,
      "learning_rate": 0.000785496183206107,
      "loss": 2.1318,
      "step": 295
    },
    {
      "epoch": 13.636363636363637,
      "grad_norm": 1.23792564868927,
      "learning_rate": 0.0007816793893129771,
      "loss": 2.1045,
      "step": 300
    },
    {
      "epoch": 13.863636363636363,
      "grad_norm": 0.9687128663063049,
      "learning_rate": 0.0007778625954198474,
      "loss": 2.1589,
      "step": 305
    },
    {
      "epoch": 14.090909090909092,
      "grad_norm": 0.997084379196167,
      "learning_rate": 0.0007740458015267175,
      "loss": 1.9725,
      "step": 310
    },
    {
      "epoch": 14.318181818181818,
      "grad_norm": 1.1493581533432007,
      "learning_rate": 0.0007702290076335879,
      "loss": 2.035,
      "step": 315
    },
    {
      "epoch": 14.545454545454545,
      "grad_norm": 1.4746767282485962,
      "learning_rate": 0.000766412213740458,
      "loss": 2.0502,
      "step": 320
    },
    {
      "epoch": 14.772727272727273,
      "grad_norm": 1.4246692657470703,
      "learning_rate": 0.0007625954198473283,
      "loss": 2.1251,
      "step": 325
    },
    {
      "epoch": 15.0,
      "grad_norm": 1.1659886837005615,
      "learning_rate": 0.0007587786259541984,
      "loss": 2.1706,
      "step": 330
    },
    {
      "epoch": 15.227272727272727,
      "grad_norm": 1.1854861974716187,
      "learning_rate": 0.0007549618320610688,
      "loss": 1.926,
      "step": 335
    },
    {
      "epoch": 15.454545454545455,
      "grad_norm": 1.4403387308120728,
      "learning_rate": 0.000751145038167939,
      "loss": 1.948,
      "step": 340
    },
    {
      "epoch": 15.681818181818182,
      "grad_norm": 1.3910669088363647,
      "learning_rate": 0.0007473282442748092,
      "loss": 2.1192,
      "step": 345
    },
    {
      "epoch": 15.909090909090908,
      "grad_norm": 1.2032103538513184,
      "learning_rate": 0.0007435114503816794,
      "loss": 2.0237,
      "step": 350
    },
    {
      "epoch": 16.136363636363637,
      "grad_norm": 1.3417693376541138,
      "learning_rate": 0.0007396946564885497,
      "loss": 1.8872,
      "step": 355
    },
    {
      "epoch": 16.363636363636363,
      "grad_norm": 1.2490097284317017,
      "learning_rate": 0.0007358778625954199,
      "loss": 1.8559,
      "step": 360
    },
    {
      "epoch": 16.59090909090909,
      "grad_norm": 1.7173535823822021,
      "learning_rate": 0.0007320610687022901,
      "loss": 2.1399,
      "step": 365
    },
    {
      "epoch": 16.818181818181817,
      "grad_norm": 1.3053690195083618,
      "learning_rate": 0.0007282442748091603,
      "loss": 2.1576,
      "step": 370
    },
    {
      "epoch": 17.045454545454547,
      "grad_norm": 1.2438676357269287,
      "learning_rate": 0.0007244274809160304,
      "loss": 1.8909,
      "step": 375
    },
    {
      "epoch": 17.272727272727273,
      "grad_norm": 1.5437941551208496,
      "learning_rate": 0.0007206106870229008,
      "loss": 1.9753,
      "step": 380
    },
    {
      "epoch": 17.5,
      "grad_norm": 1.3267649412155151,
      "learning_rate": 0.0007167938931297711,
      "loss": 1.9344,
      "step": 385
    },
    {
      "epoch": 17.727272727272727,
      "grad_norm": 1.3231066465377808,
      "learning_rate": 0.0007129770992366412,
      "loss": 2.0565,
      "step": 390
    },
    {
      "epoch": 17.954545454545453,
      "grad_norm": 1.1888983249664307,
      "learning_rate": 0.0007091603053435115,
      "loss": 1.9334,
      "step": 395
    },
    {
      "epoch": 18.181818181818183,
      "grad_norm": 1.1468756198883057,
      "learning_rate": 0.0007053435114503817,
      "loss": 1.8883,
      "step": 400
    },
    {
      "epoch": 18.40909090909091,
      "grad_norm": 1.3033207654953003,
      "learning_rate": 0.000701526717557252,
      "loss": 2.1138,
      "step": 405
    },
    {
      "epoch": 18.636363636363637,
      "grad_norm": 1.5269049406051636,
      "learning_rate": 0.0006977099236641221,
      "loss": 1.8162,
      "step": 410
    },
    {
      "epoch": 18.863636363636363,
      "grad_norm": 1.385398268699646,
      "learning_rate": 0.0006938931297709924,
      "loss": 1.9355,
      "step": 415
    },
    {
      "epoch": 19.09090909090909,
      "grad_norm": 1.1283581256866455,
      "learning_rate": 0.0006900763358778626,
      "loss": 1.797,
      "step": 420
    },
    {
      "epoch": 19.318181818181817,
      "grad_norm": 1.612119436264038,
      "learning_rate": 0.0006862595419847329,
      "loss": 1.9754,
      "step": 425
    },
    {
      "epoch": 19.545454545454547,
      "grad_norm": 1.258583903312683,
      "learning_rate": 0.0006824427480916031,
      "loss": 2.0111,
      "step": 430
    },
    {
      "epoch": 19.772727272727273,
      "grad_norm": 1.228973627090454,
      "learning_rate": 0.0006786259541984732,
      "loss": 1.7781,
      "step": 435
    },
    {
      "epoch": 20.0,
      "grad_norm": 1.5443840026855469,
      "learning_rate": 0.0006748091603053436,
      "loss": 1.9047,
      "step": 440
    },
    {
      "epoch": 20.227272727272727,
      "grad_norm": 1.1150002479553223,
      "learning_rate": 0.0006709923664122138,
      "loss": 1.9179,
      "step": 445
    },
    {
      "epoch": 20.454545454545453,
      "grad_norm": 1.3786147832870483,
      "learning_rate": 0.000667175572519084,
      "loss": 1.7974,
      "step": 450
    },
    {
      "epoch": 20.681818181818183,
      "grad_norm": 1.1578296422958374,
      "learning_rate": 0.0006633587786259541,
      "loss": 1.923,
      "step": 455
    },
    {
      "epoch": 20.90909090909091,
      "grad_norm": 1.7487417459487915,
      "learning_rate": 0.0006595419847328245,
      "loss": 1.8362,
      "step": 460
    },
    {
      "epoch": 21.136363636363637,
      "grad_norm": 1.4229291677474976,
      "learning_rate": 0.0006557251908396946,
      "loss": 1.8349,
      "step": 465
    },
    {
      "epoch": 21.363636363636363,
      "grad_norm": 1.426033854484558,
      "learning_rate": 0.0006519083969465649,
      "loss": 1.8702,
      "step": 470
    },
    {
      "epoch": 21.59090909090909,
      "grad_norm": 1.387900710105896,
      "learning_rate": 0.0006480916030534352,
      "loss": 1.7737,
      "step": 475
    },
    {
      "epoch": 21.818181818181817,
      "grad_norm": 1.292176365852356,
      "learning_rate": 0.0006442748091603053,
      "loss": 1.8286,
      "step": 480
    },
    {
      "epoch": 22.045454545454547,
      "grad_norm": 1.3004021644592285,
      "learning_rate": 0.0006404580152671757,
      "loss": 1.7436,
      "step": 485
    },
    {
      "epoch": 22.272727272727273,
      "grad_norm": 1.8281480073928833,
      "learning_rate": 0.0006366412213740458,
      "loss": 1.9895,
      "step": 490
    },
    {
      "epoch": 22.5,
      "grad_norm": 1.5499560832977295,
      "learning_rate": 0.000632824427480916,
      "loss": 1.5438,
      "step": 495
    },
    {
      "epoch": 22.727272727272727,
      "grad_norm": 1.2997480630874634,
      "learning_rate": 0.0006290076335877862,
      "loss": 1.9039,
      "step": 500
    },
    {
      "epoch": 22.954545454545453,
      "grad_norm": 1.495361328125,
      "learning_rate": 0.0006251908396946565,
      "loss": 1.8852,
      "step": 505
    },
    {
      "epoch": 23.181818181818183,
      "grad_norm": 1.5046411752700806,
      "learning_rate": 0.0006213740458015267,
      "loss": 1.7558,
      "step": 510
    },
    {
      "epoch": 23.40909090909091,
      "grad_norm": 1.2714438438415527,
      "learning_rate": 0.0006175572519083969,
      "loss": 1.6744,
      "step": 515
    },
    {
      "epoch": 23.636363636363637,
      "grad_norm": 1.3774927854537964,
      "learning_rate": 0.0006137404580152672,
      "loss": 1.7238,
      "step": 520
    },
    {
      "epoch": 23.863636363636363,
      "grad_norm": 1.611641764640808,
      "learning_rate": 0.0006099236641221374,
      "loss": 2.0104,
      "step": 525
    },
    {
      "epoch": 24.09090909090909,
      "grad_norm": 1.3492189645767212,
      "learning_rate": 0.0006061068702290077,
      "loss": 1.5668,
      "step": 530
    },
    {
      "epoch": 24.318181818181817,
      "grad_norm": 1.5120183229446411,
      "learning_rate": 0.0006022900763358778,
      "loss": 1.9433,
      "step": 535
    },
    {
      "epoch": 24.545454545454547,
      "grad_norm": 1.1622477769851685,
      "learning_rate": 0.0005984732824427481,
      "loss": 1.7414,
      "step": 540
    },
    {
      "epoch": 24.772727272727273,
      "grad_norm": 1.400715947151184,
      "learning_rate": 0.0005946564885496183,
      "loss": 1.8383,
      "step": 545
    },
    {
      "epoch": 25.0,
      "grad_norm": 1.5961432456970215,
      "learning_rate": 0.0005908396946564886,
      "loss": 1.5459,
      "step": 550
    },
    {
      "epoch": 25.227272727272727,
      "grad_norm": 1.3233859539031982,
      "learning_rate": 0.0005870229007633587,
      "loss": 1.7535,
      "step": 555
    },
    {
      "epoch": 25.454545454545453,
      "grad_norm": 1.1325119733810425,
      "learning_rate": 0.000583206106870229,
      "loss": 1.6652,
      "step": 560
    },
    {
      "epoch": 25.681818181818183,
      "grad_norm": 1.6162890195846558,
      "learning_rate": 0.0005793893129770992,
      "loss": 1.6515,
      "step": 565
    },
    {
      "epoch": 25.90909090909091,
      "grad_norm": 1.3626761436462402,
      "learning_rate": 0.0005755725190839695,
      "loss": 1.7922,
      "step": 570
    },
    {
      "epoch": 26.136363636363637,
      "grad_norm": 1.6542892456054688,
      "learning_rate": 0.0005717557251908397,
      "loss": 1.7039,
      "step": 575
    },
    {
      "epoch": 26.363636363636363,
      "grad_norm": 1.1657252311706543,
      "learning_rate": 0.0005679389312977099,
      "loss": 1.5545,
      "step": 580
    },
    {
      "epoch": 26.59090909090909,
      "grad_norm": 1.4741870164871216,
      "learning_rate": 0.0005641221374045801,
      "loss": 1.8595,
      "step": 585
    },
    {
      "epoch": 26.818181818181817,
      "grad_norm": 1.3544542789459229,
      "learning_rate": 0.0005603053435114504,
      "loss": 1.7351,
      "step": 590
    },
    {
      "epoch": 27.045454545454547,
      "grad_norm": 1.264417052268982,
      "learning_rate": 0.0005564885496183206,
      "loss": 1.602,
      "step": 595
    },
    {
      "epoch": 27.272727272727273,
      "grad_norm": 1.374471664428711,
      "learning_rate": 0.0005526717557251909,
      "loss": 1.7263,
      "step": 600
    },
    {
      "epoch": 27.5,
      "grad_norm": 1.7586982250213623,
      "learning_rate": 0.000548854961832061,
      "loss": 1.48,
      "step": 605
    },
    {
      "epoch": 27.727272727272727,
      "grad_norm": 1.5584664344787598,
      "learning_rate": 0.0005450381679389314,
      "loss": 1.752,
      "step": 610
    },
    {
      "epoch": 27.954545454545453,
      "grad_norm": 1.89519464969635,
      "learning_rate": 0.0005412213740458015,
      "loss": 1.7,
      "step": 615
    },
    {
      "epoch": 28.181818181818183,
      "grad_norm": 1.4505727291107178,
      "learning_rate": 0.0005374045801526718,
      "loss": 1.7042,
      "step": 620
    },
    {
      "epoch": 28.40909090909091,
      "grad_norm": 1.1566518545150757,
      "learning_rate": 0.0005335877862595419,
      "loss": 1.5901,
      "step": 625
    },
    {
      "epoch": 28.636363636363637,
      "grad_norm": 1.632746934890747,
      "learning_rate": 0.0005297709923664123,
      "loss": 1.5566,
      "step": 630
    },
    {
      "epoch": 28.863636363636363,
      "grad_norm": 2.365321636199951,
      "learning_rate": 0.0005259541984732824,
      "loss": 1.8387,
      "step": 635
    },
    {
      "epoch": 29.09090909090909,
      "grad_norm": 1.549706220626831,
      "learning_rate": 0.0005221374045801527,
      "loss": 1.5472,
      "step": 640
    },
    {
      "epoch": 29.318181818181817,
      "grad_norm": 1.6573525667190552,
      "learning_rate": 0.0005183206106870229,
      "loss": 1.6396,
      "step": 645
    },
    {
      "epoch": 29.545454545454547,
      "grad_norm": 1.5533396005630493,
      "learning_rate": 0.0005145038167938931,
      "loss": 1.6223,
      "step": 650
    },
    {
      "epoch": 29.772727272727273,
      "grad_norm": 1.4663163423538208,
      "learning_rate": 0.0005106870229007634,
      "loss": 1.6016,
      "step": 655
    },
    {
      "epoch": 30.0,
      "grad_norm": 1.8668919801712036,
      "learning_rate": 0.0005068702290076336,
      "loss": 1.6512,
      "step": 660
    },
    {
      "epoch": 30.227272727272727,
      "grad_norm": 1.1868700981140137,
      "learning_rate": 0.0005030534351145038,
      "loss": 1.5318,
      "step": 665
    },
    {
      "epoch": 30.454545454545453,
      "grad_norm": 1.7088485956192017,
      "learning_rate": 0.0004992366412213741,
      "loss": 1.6292,
      "step": 670
    },
    {
      "epoch": 30.681818181818183,
      "grad_norm": 1.3804155588150024,
      "learning_rate": 0.0004954198473282443,
      "loss": 1.4105,
      "step": 675
    },
    {
      "epoch": 30.90909090909091,
      "grad_norm": 1.485408902168274,
      "learning_rate": 0.0004916030534351145,
      "loss": 1.694,
      "step": 680
    },
    {
      "epoch": 31.136363636363637,
      "grad_norm": 1.3754496574401855,
      "learning_rate": 0.00048778625954198473,
      "loss": 1.748,
      "step": 685
    },
    {
      "epoch": 31.363636363636363,
      "grad_norm": 1.2672978639602661,
      "learning_rate": 0.000483969465648855,
      "loss": 1.5879,
      "step": 690
    },
    {
      "epoch": 31.59090909090909,
      "grad_norm": 1.823634386062622,
      "learning_rate": 0.0004801526717557252,
      "loss": 1.4978,
      "step": 695
    },
    {
      "epoch": 31.818181818181817,
      "grad_norm": 1.4057884216308594,
      "learning_rate": 0.00047633587786259543,
      "loss": 1.5974,
      "step": 700
    },
    {
      "epoch": 32.04545454545455,
      "grad_norm": 1.6209375858306885,
      "learning_rate": 0.0004725190839694656,
      "loss": 1.5986,
      "step": 705
    },
    {
      "epoch": 32.27272727272727,
      "grad_norm": 1.7151137590408325,
      "learning_rate": 0.00046870229007633593,
      "loss": 1.5372,
      "step": 710
    },
    {
      "epoch": 32.5,
      "grad_norm": 1.267319679260254,
      "learning_rate": 0.0004648854961832061,
      "loss": 1.6668,
      "step": 715
    },
    {
      "epoch": 32.72727272727273,
      "grad_norm": 1.554396390914917,
      "learning_rate": 0.0004610687022900764,
      "loss": 1.4483,
      "step": 720
    },
    {
      "epoch": 32.95454545454545,
      "grad_norm": 5.723525524139404,
      "learning_rate": 0.0004572519083969466,
      "loss": 1.5957,
      "step": 725
    },
    {
      "epoch": 33.18181818181818,
      "grad_norm": 1.5457714796066284,
      "learning_rate": 0.0004534351145038168,
      "loss": 1.5037,
      "step": 730
    },
    {
      "epoch": 33.40909090909091,
      "grad_norm": 1.3847373723983765,
      "learning_rate": 0.000449618320610687,
      "loss": 1.66,
      "step": 735
    },
    {
      "epoch": 33.63636363636363,
      "grad_norm": 1.5147931575775146,
      "learning_rate": 0.0004458015267175573,
      "loss": 1.5284,
      "step": 740
    },
    {
      "epoch": 33.86363636363637,
      "grad_norm": 1.6915524005889893,
      "learning_rate": 0.00044198473282442747,
      "loss": 1.5324,
      "step": 745
    },
    {
      "epoch": 34.09090909090909,
      "grad_norm": 1.5066893100738525,
      "learning_rate": 0.00043816793893129767,
      "loss": 1.4443,
      "step": 750
    },
    {
      "epoch": 34.31818181818182,
      "grad_norm": 1.455224871635437,
      "learning_rate": 0.000434351145038168,
      "loss": 1.4551,
      "step": 755
    },
    {
      "epoch": 34.54545454545455,
      "grad_norm": 1.6076053380966187,
      "learning_rate": 0.00043053435114503817,
      "loss": 1.6989,
      "step": 760
    },
    {
      "epoch": 34.77272727272727,
      "grad_norm": 1.452947735786438,
      "learning_rate": 0.0004267175572519084,
      "loss": 1.5339,
      "step": 765
    },
    {
      "epoch": 35.0,
      "grad_norm": 1.5959230661392212,
      "learning_rate": 0.0004229007633587786,
      "loss": 1.5581,
      "step": 770
    },
    {
      "epoch": 35.22727272727273,
      "grad_norm": 1.3989920616149902,
      "learning_rate": 0.00041908396946564887,
      "loss": 1.5282,
      "step": 775
    },
    {
      "epoch": 35.45454545454545,
      "grad_norm": 1.6332902908325195,
      "learning_rate": 0.00041526717557251907,
      "loss": 1.5942,
      "step": 780
    },
    {
      "epoch": 35.68181818181818,
      "grad_norm": 1.6795079708099365,
      "learning_rate": 0.0004114503816793893,
      "loss": 1.3642,
      "step": 785
    },
    {
      "epoch": 35.90909090909091,
      "grad_norm": 1.3565510511398315,
      "learning_rate": 0.0004076335877862595,
      "loss": 1.5512,
      "step": 790
    },
    {
      "epoch": 36.13636363636363,
      "grad_norm": 1.742193579673767,
      "learning_rate": 0.00040381679389312977,
      "loss": 1.5142,
      "step": 795
    },
    {
      "epoch": 36.36363636363637,
      "grad_norm": 1.4210690259933472,
      "learning_rate": 0.0004,
      "loss": 1.5273,
      "step": 800
    },
    {
      "epoch": 36.59090909090909,
      "grad_norm": 1.5284607410430908,
      "learning_rate": 0.00039618320610687027,
      "loss": 1.4742,
      "step": 805
    },
    {
      "epoch": 36.81818181818182,
      "grad_norm": 1.458203673362732,
      "learning_rate": 0.00039236641221374047,
      "loss": 1.3699,
      "step": 810
    },
    {
      "epoch": 37.04545454545455,
      "grad_norm": 1.680264949798584,
      "learning_rate": 0.0003885496183206107,
      "loss": 1.6223,
      "step": 815
    },
    {
      "epoch": 37.27272727272727,
      "grad_norm": 1.4183882474899292,
      "learning_rate": 0.0003847328244274809,
      "loss": 1.5347,
      "step": 820
    },
    {
      "epoch": 37.5,
      "grad_norm": 1.5959917306900024,
      "learning_rate": 0.00038091603053435117,
      "loss": 1.3715,
      "step": 825
    },
    {
      "epoch": 37.72727272727273,
      "grad_norm": 2.1545746326446533,
      "learning_rate": 0.00037709923664122136,
      "loss": 1.5804,
      "step": 830
    },
    {
      "epoch": 37.95454545454545,
      "grad_norm": 1.2770241498947144,
      "learning_rate": 0.0003732824427480916,
      "loss": 1.5099,
      "step": 835
    },
    {
      "epoch": 38.18181818181818,
      "grad_norm": 1.5484203100204468,
      "learning_rate": 0.00036946564885496187,
      "loss": 1.5524,
      "step": 840
    },
    {
      "epoch": 38.40909090909091,
      "grad_norm": 2.401808261871338,
      "learning_rate": 0.00036564885496183206,
      "loss": 1.3956,
      "step": 845
    },
    {
      "epoch": 38.63636363636363,
      "grad_norm": 1.4565317630767822,
      "learning_rate": 0.0003618320610687023,
      "loss": 1.444,
      "step": 850
    },
    {
      "epoch": 38.86363636363637,
      "grad_norm": 1.544919729232788,
      "learning_rate": 0.0003580152671755725,
      "loss": 1.509,
      "step": 855
    },
    {
      "epoch": 39.09090909090909,
      "grad_norm": 1.3579338788986206,
      "learning_rate": 0.00035419847328244276,
      "loss": 1.6576,
      "step": 860
    },
    {
      "epoch": 39.31818181818182,
      "grad_norm": 1.8130964040756226,
      "learning_rate": 0.00035038167938931296,
      "loss": 1.6068,
      "step": 865
    },
    {
      "epoch": 39.54545454545455,
      "grad_norm": 1.693963646888733,
      "learning_rate": 0.0003465648854961832,
      "loss": 1.3828,
      "step": 870
    },
    {
      "epoch": 39.77272727272727,
      "grad_norm": 1.6127667427062988,
      "learning_rate": 0.0003427480916030534,
      "loss": 1.4268,
      "step": 875
    },
    {
      "epoch": 40.0,
      "grad_norm": 1.6086705923080444,
      "learning_rate": 0.00033893129770992366,
      "loss": 1.4533,
      "step": 880
    },
    {
      "epoch": 40.22727272727273,
      "grad_norm": 1.466684341430664,
      "learning_rate": 0.0003351145038167939,
      "loss": 1.4864,
      "step": 885
    },
    {
      "epoch": 40.45454545454545,
      "grad_norm": 1.936419129371643,
      "learning_rate": 0.00033129770992366416,
      "loss": 1.4147,
      "step": 890
    },
    {
      "epoch": 40.68181818181818,
      "grad_norm": 2.043912887573242,
      "learning_rate": 0.00032748091603053436,
      "loss": 1.4195,
      "step": 895
    },
    {
      "epoch": 40.90909090909091,
      "grad_norm": 1.695928931236267,
      "learning_rate": 0.0003236641221374046,
      "loss": 1.4971,
      "step": 900
    },
    {
      "epoch": 41.13636363636363,
      "grad_norm": 1.472865104675293,
      "learning_rate": 0.0003198473282442748,
      "loss": 1.4874,
      "step": 905
    },
    {
      "epoch": 41.36363636363637,
      "grad_norm": 1.548599362373352,
      "learning_rate": 0.00031603053435114506,
      "loss": 1.4326,
      "step": 910
    },
    {
      "epoch": 41.59090909090909,
      "grad_norm": 1.3973685503005981,
      "learning_rate": 0.00031221374045801526,
      "loss": 1.3493,
      "step": 915
    },
    {
      "epoch": 41.81818181818182,
      "grad_norm": 1.519767165184021,
      "learning_rate": 0.0003083969465648855,
      "loss": 1.5108,
      "step": 920
    },
    {
      "epoch": 42.04545454545455,
      "grad_norm": 1.5931103229522705,
      "learning_rate": 0.0003045801526717557,
      "loss": 1.4931,
      "step": 925
    },
    {
      "epoch": 42.27272727272727,
      "grad_norm": 1.381412148475647,
      "learning_rate": 0.000300763358778626,
      "loss": 1.3236,
      "step": 930
    },
    {
      "epoch": 42.5,
      "grad_norm": 1.6794276237487793,
      "learning_rate": 0.0002969465648854962,
      "loss": 1.5387,
      "step": 935
    },
    {
      "epoch": 42.72727272727273,
      "grad_norm": 1.4030767679214478,
      "learning_rate": 0.0002931297709923664,
      "loss": 1.4133,
      "step": 940
    },
    {
      "epoch": 42.95454545454545,
      "grad_norm": 1.277978539466858,
      "learning_rate": 0.00028931297709923666,
      "loss": 1.4286,
      "step": 945
    },
    {
      "epoch": 43.18181818181818,
      "grad_norm": 1.3384058475494385,
      "learning_rate": 0.00028549618320610685,
      "loss": 1.286,
      "step": 950
    },
    {
      "epoch": 43.40909090909091,
      "grad_norm": 1.542131781578064,
      "learning_rate": 0.0002816793893129771,
      "loss": 1.3711,
      "step": 955
    },
    {
      "epoch": 43.63636363636363,
      "grad_norm": 1.9281657934188843,
      "learning_rate": 0.0002778625954198473,
      "loss": 1.4651,
      "step": 960
    },
    {
      "epoch": 43.86363636363637,
      "grad_norm": 1.6687359809875488,
      "learning_rate": 0.00027404580152671755,
      "loss": 1.2989,
      "step": 965
    },
    {
      "epoch": 44.09090909090909,
      "grad_norm": 2.545691967010498,
      "learning_rate": 0.00027022900763358775,
      "loss": 1.5085,
      "step": 970
    },
    {
      "epoch": 44.31818181818182,
      "grad_norm": 1.458228349685669,
      "learning_rate": 0.00026641221374045805,
      "loss": 1.4408,
      "step": 975
    },
    {
      "epoch": 44.54545454545455,
      "grad_norm": 1.4819468259811401,
      "learning_rate": 0.00026259541984732825,
      "loss": 1.4194,
      "step": 980
    },
    {
      "epoch": 44.77272727272727,
      "grad_norm": 1.5038923025131226,
      "learning_rate": 0.0002587786259541985,
      "loss": 1.422,
      "step": 985
    },
    {
      "epoch": 45.0,
      "grad_norm": 2.122556209564209,
      "learning_rate": 0.0002549618320610687,
      "loss": 1.4886,
      "step": 990
    },
    {
      "epoch": 45.22727272727273,
      "grad_norm": 1.6474729776382446,
      "learning_rate": 0.00025114503816793895,
      "loss": 1.3923,
      "step": 995
    },
    {
      "epoch": 45.45454545454545,
      "grad_norm": 1.414453387260437,
      "learning_rate": 0.00024732824427480915,
      "loss": 1.46,
      "step": 1000
    },
    {
      "epoch": 45.68181818181818,
      "grad_norm": 1.4807459115982056,
      "learning_rate": 0.0002435114503816794,
      "loss": 1.3681,
      "step": 1005
    },
    {
      "epoch": 45.90909090909091,
      "grad_norm": 1.4684728384017944,
      "learning_rate": 0.00023969465648854962,
      "loss": 1.3723,
      "step": 1010
    },
    {
      "epoch": 46.13636363636363,
      "grad_norm": 1.5204261541366577,
      "learning_rate": 0.00023587786259541985,
      "loss": 1.3186,
      "step": 1015
    },
    {
      "epoch": 46.36363636363637,
      "grad_norm": 1.5188096761703491,
      "learning_rate": 0.00023206106870229007,
      "loss": 1.3504,
      "step": 1020
    },
    {
      "epoch": 46.59090909090909,
      "grad_norm": 1.6082470417022705,
      "learning_rate": 0.00022824427480916032,
      "loss": 1.5202,
      "step": 1025
    },
    {
      "epoch": 46.81818181818182,
      "grad_norm": 1.3123931884765625,
      "learning_rate": 0.00022442748091603055,
      "loss": 1.2749,
      "step": 1030
    },
    {
      "epoch": 47.04545454545455,
      "grad_norm": 1.7690349817276,
      "learning_rate": 0.00022061068702290077,
      "loss": 1.4617,
      "step": 1035
    },
    {
      "epoch": 47.27272727272727,
      "grad_norm": 1.1697356700897217,
      "learning_rate": 0.000216793893129771,
      "loss": 1.3325,
      "step": 1040
    },
    {
      "epoch": 47.5,
      "grad_norm": 1.785408854484558,
      "learning_rate": 0.00021297709923664122,
      "loss": 1.3284,
      "step": 1045
    },
    {
      "epoch": 47.72727272727273,
      "grad_norm": 1.7825535535812378,
      "learning_rate": 0.00020916030534351147,
      "loss": 1.3469,
      "step": 1050
    },
    {
      "epoch": 47.95454545454545,
      "grad_norm": 1.5467815399169922,
      "learning_rate": 0.0002053435114503817,
      "loss": 1.3051,
      "step": 1055
    },
    {
      "epoch": 48.18181818181818,
      "grad_norm": 1.4680105447769165,
      "learning_rate": 0.00020152671755725192,
      "loss": 1.2052,
      "step": 1060
    },
    {
      "epoch": 48.40909090909091,
      "grad_norm": 1.607184886932373,
      "learning_rate": 0.00019770992366412214,
      "loss": 1.4571,
      "step": 1065
    },
    {
      "epoch": 48.63636363636363,
      "grad_norm": 1.5240612030029297,
      "learning_rate": 0.0001938931297709924,
      "loss": 1.4502,
      "step": 1070
    },
    {
      "epoch": 48.86363636363637,
      "grad_norm": 1.7150813341140747,
      "learning_rate": 0.0001900763358778626,
      "loss": 1.3044,
      "step": 1075
    },
    {
      "epoch": 49.09090909090909,
      "grad_norm": 1.4667779207229614,
      "learning_rate": 0.00018625954198473282,
      "loss": 1.2584,
      "step": 1080
    },
    {
      "epoch": 49.31818181818182,
      "grad_norm": 1.4980791807174683,
      "learning_rate": 0.00018244274809160304,
      "loss": 1.4968,
      "step": 1085
    },
    {
      "epoch": 49.54545454545455,
      "grad_norm": 1.9735945463180542,
      "learning_rate": 0.00017862595419847326,
      "loss": 1.3293,
      "step": 1090
    },
    {
      "epoch": 49.77272727272727,
      "grad_norm": 1.6466201543807983,
      "learning_rate": 0.00017480916030534352,
      "loss": 1.2345,
      "step": 1095
    },
    {
      "epoch": 50.0,
      "grad_norm": 1.6536972522735596,
      "learning_rate": 0.00017099236641221374,
      "loss": 1.3818,
      "step": 1100
    },
    {
      "epoch": 50.22727272727273,
      "grad_norm": 1.3234857320785522,
      "learning_rate": 0.00016717557251908396,
      "loss": 1.2182,
      "step": 1105
    },
    {
      "epoch": 50.45454545454545,
      "grad_norm": 1.7137891054153442,
      "learning_rate": 0.0001633587786259542,
      "loss": 1.4368,
      "step": 1110
    },
    {
      "epoch": 50.68181818181818,
      "grad_norm": 1.6651742458343506,
      "learning_rate": 0.00015954198473282444,
      "loss": 1.3356,
      "step": 1115
    },
    {
      "epoch": 50.90909090909091,
      "grad_norm": 1.6156752109527588,
      "learning_rate": 0.00015572519083969466,
      "loss": 1.4884,
      "step": 1120
    },
    {
      "epoch": 51.13636363636363,
      "grad_norm": 1.5818337202072144,
      "learning_rate": 0.0001519083969465649,
      "loss": 1.2424,
      "step": 1125
    },
    {
      "epoch": 51.36363636363637,
      "grad_norm": 1.337407112121582,
      "learning_rate": 0.0001480916030534351,
      "loss": 1.2156,
      "step": 1130
    },
    {
      "epoch": 51.59090909090909,
      "grad_norm": 1.5922741889953613,
      "learning_rate": 0.00014427480916030536,
      "loss": 1.3866,
      "step": 1135
    },
    {
      "epoch": 51.81818181818182,
      "grad_norm": 1.6388413906097412,
      "learning_rate": 0.0001404580152671756,
      "loss": 1.3607,
      "step": 1140
    },
    {
      "epoch": 52.04545454545455,
      "grad_norm": 1.7943936586380005,
      "learning_rate": 0.0001366412213740458,
      "loss": 1.4207,
      "step": 1145
    },
    {
      "epoch": 52.27272727272727,
      "grad_norm": 1.4611494541168213,
      "learning_rate": 0.00013282442748091604,
      "loss": 1.347,
      "step": 1150
    },
    {
      "epoch": 52.5,
      "grad_norm": 1.6876729726791382,
      "learning_rate": 0.00012900763358778626,
      "loss": 1.3941,
      "step": 1155
    },
    {
      "epoch": 52.72727272727273,
      "grad_norm": 1.6493033170700073,
      "learning_rate": 0.0001251908396946565,
      "loss": 1.4099,
      "step": 1160
    },
    {
      "epoch": 52.95454545454545,
      "grad_norm": 1.5811717510223389,
      "learning_rate": 0.00012137404580152672,
      "loss": 1.291,
      "step": 1165
    },
    {
      "epoch": 53.18181818181818,
      "grad_norm": 1.7274588346481323,
      "learning_rate": 0.00011755725190839695,
      "loss": 1.3303,
      "step": 1170
    },
    {
      "epoch": 53.40909090909091,
      "grad_norm": 1.446905255317688,
      "learning_rate": 0.00011374045801526718,
      "loss": 1.2788,
      "step": 1175
    },
    {
      "epoch": 53.63636363636363,
      "grad_norm": 1.5425688028335571,
      "learning_rate": 0.00010992366412213741,
      "loss": 1.2903,
      "step": 1180
    },
    {
      "epoch": 53.86363636363637,
      "grad_norm": 1.5360238552093506,
      "learning_rate": 0.00010610687022900765,
      "loss": 1.3568,
      "step": 1185
    },
    {
      "epoch": 54.09090909090909,
      "grad_norm": 1.3288236856460571,
      "learning_rate": 0.00010229007633587786,
      "loss": 1.3235,
      "step": 1190
    },
    {
      "epoch": 54.31818181818182,
      "grad_norm": 1.3450392484664917,
      "learning_rate": 9.84732824427481e-05,
      "loss": 1.1246,
      "step": 1195
    },
    {
      "epoch": 54.54545454545455,
      "grad_norm": 1.7516437768936157,
      "learning_rate": 9.465648854961832e-05,
      "loss": 1.3869,
      "step": 1200
    },
    {
      "epoch": 54.77272727272727,
      "grad_norm": 1.5458791255950928,
      "learning_rate": 9.083969465648854e-05,
      "loss": 1.4257,
      "step": 1205
    },
    {
      "epoch": 55.0,
      "grad_norm": 1.9212504625320435,
      "learning_rate": 8.702290076335878e-05,
      "loss": 1.2702,
      "step": 1210
    },
    {
      "epoch": 55.22727272727273,
      "grad_norm": 1.5778807401657104,
      "learning_rate": 8.3206106870229e-05,
      "loss": 1.3829,
      "step": 1215
    },
    {
      "epoch": 55.45454545454545,
      "grad_norm": 1.5754426717758179,
      "learning_rate": 7.938931297709924e-05,
      "loss": 1.3241,
      "step": 1220
    },
    {
      "epoch": 55.68181818181818,
      "grad_norm": 1.6603679656982422,
      "learning_rate": 7.557251908396947e-05,
      "loss": 1.3231,
      "step": 1225
    },
    {
      "epoch": 55.90909090909091,
      "grad_norm": 1.4736273288726807,
      "learning_rate": 7.17557251908397e-05,
      "loss": 1.3192,
      "step": 1230
    },
    {
      "epoch": 56.13636363636363,
      "grad_norm": 1.640984296798706,
      "learning_rate": 6.793893129770991e-05,
      "loss": 1.2868,
      "step": 1235
    },
    {
      "epoch": 56.36363636363637,
      "grad_norm": 1.5882110595703125,
      "learning_rate": 6.412213740458015e-05,
      "loss": 1.2681,
      "step": 1240
    },
    {
      "epoch": 56.59090909090909,
      "grad_norm": 1.5223541259765625,
      "learning_rate": 6.0305343511450384e-05,
      "loss": 1.4196,
      "step": 1245
    },
    {
      "epoch": 56.81818181818182,
      "grad_norm": 1.8087632656097412,
      "learning_rate": 5.648854961832061e-05,
      "loss": 1.3092,
      "step": 1250
    },
    {
      "epoch": 57.04545454545455,
      "grad_norm": 1.6870009899139404,
      "learning_rate": 5.267175572519084e-05,
      "loss": 1.3017,
      "step": 1255
    },
    {
      "epoch": 57.27272727272727,
      "grad_norm": 1.390067219734192,
      "learning_rate": 4.885496183206107e-05,
      "loss": 1.2121,
      "step": 1260
    },
    {
      "epoch": 57.5,
      "grad_norm": 1.552931547164917,
      "learning_rate": 4.50381679389313e-05,
      "loss": 1.4293,
      "step": 1265
    },
    {
      "epoch": 57.72727272727273,
      "grad_norm": 1.4588959217071533,
      "learning_rate": 4.122137404580153e-05,
      "loss": 1.3373,
      "step": 1270
    },
    {
      "epoch": 57.95454545454545,
      "grad_norm": 1.4109493494033813,
      "learning_rate": 3.7404580152671756e-05,
      "loss": 1.1825,
      "step": 1275
    },
    {
      "epoch": 58.18181818181818,
      "grad_norm": 1.4750897884368896,
      "learning_rate": 3.358778625954199e-05,
      "loss": 1.2561,
      "step": 1280
    },
    {
      "epoch": 58.40909090909091,
      "grad_norm": 1.3289991617202759,
      "learning_rate": 2.9770992366412214e-05,
      "loss": 1.3368,
      "step": 1285
    },
    {
      "epoch": 58.63636363636363,
      "grad_norm": 1.4145376682281494,
      "learning_rate": 2.5954198473282442e-05,
      "loss": 1.4336,
      "step": 1290
    },
    {
      "epoch": 58.86363636363637,
      "grad_norm": 1.5919111967086792,
      "learning_rate": 2.2137404580152673e-05,
      "loss": 1.229,
      "step": 1295
    },
    {
      "epoch": 59.09090909090909,
      "grad_norm": 1.6197015047073364,
      "learning_rate": 1.83206106870229e-05,
      "loss": 1.3162,
      "step": 1300
    },
    {
      "epoch": 59.31818181818182,
      "grad_norm": 1.3380624055862427,
      "learning_rate": 1.450381679389313e-05,
      "loss": 1.3513,
      "step": 1305
    },
    {
      "epoch": 59.54545454545455,
      "grad_norm": 1.73191499710083,
      "learning_rate": 1.0687022900763359e-05,
      "loss": 1.2587,
      "step": 1310
    },
    {
      "epoch": 59.77272727272727,
      "grad_norm": 1.6469608545303345,
      "learning_rate": 6.870229007633588e-06,
      "loss": 1.2609,
      "step": 1315
    },
    {
      "epoch": 60.0,
      "grad_norm": 2.171546220779419,
      "learning_rate": 3.053435114503817e-06,
      "loss": 1.2971,
      "step": 1320
    }
  ],
  "logging_steps": 5,
  "max_steps": 1320,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 60,
  "save_steps": 500,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": true
      },
      "attributes": {}
    }
  },
  "total_flos": 1431885053952000.0,
  "train_batch_size": 4,
  "trial_name": null,
  "trial_params": null
}
